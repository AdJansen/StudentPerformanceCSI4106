{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FN score he got was around 90% in macro, more then 80\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV\n",
    "import matplotlib.pyplot as plot\n",
    "\n",
    "# we can use the LabelEncoder to encode the gender feature\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, LabelBinarizer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit, cross_val_score, GridSearchCV, cross_validate\n",
    "\n",
    "# importing two different imputation methods that take into consideration all the features when predicting the missing values\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#heatmap\n",
    "import seaborn as sns\n",
    "\n",
    "#multiclass imports\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.dummy import DummyClassifier #Will identify the maority calss base line, model needs to do better then the baseline\n",
    "\n",
    "# oversample the minority class using SMOTE\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from collections import Counter\n",
    "\n",
    "# to reduce randomness then you put the seed\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "Data shape: \n",
      "(145, 31)\n",
      "\n",
      "Data size: \n",
      "4495\n",
      "\n",
      "Data ndim: \n",
      "2\n",
      "\n",
      "_____________________________________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/student_prediction.csv\")\n",
    "\n",
    "df = df.rename(columns = {'KIDS':'PARENT_STATUS'}) #There is a column name error in the data noted in the Kaggle description, this fixes it.\n",
    "df = df.drop([\"STUDENTID\", \"COURSE ID\"], axis=1)\n",
    "\n",
    "gathered_df = pd.read_csv(\"./data/Higher Education Students Performance Evaluation.csv\")\n",
    "gathered_df = gathered_df.rename(columns = {'KIDS':'PARENT_STATUS'})\n",
    "res = list(set(df).difference(set(gathered_df)))\n",
    "print(res)\n",
    "#df = pd.concat([df, gathered_df], axis=0)\n",
    "print(f\"Data shape: \\n{df.shape}\\n\")\n",
    "print(f\"Data size: \\n{df.size}\\n\")\n",
    "print(f\"Data ndim: \\n{df.ndim}\\n\")\n",
    "print(\"_____________________________________________\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAD4CAYAAADFAawfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAMeklEQVR4nO3dfYhld33H8ffHPJCYLLHtphdJQkdbDWptEnuxlFQ7m6DERqqFFrPQFoswtfQhglC3f5QiFLqFUiolRYfWNqVqEDVSHLsquIdEMIm7adLmyaLJpiatjaFtkgmhSeTbP+ZOMpnczZzZvefub5z3C4bM3Hvuud8Lh3cOvz333lQVkqR2vexUDyBJemmGWpIaZ6glqXGGWpIaZ6glqXGnD7HTvXv31sLCwhC7lk7Kk08+yTnnnHOqx5Be5OjRo49W1fnT7hsk1AsLCxw5cmSIXUsnpes6FhcXT/UY0oskefB497n0IUmNM9SS1DhDLUmNM9SS1DhDLUmN2zLUSS5OcseGn8eTfGAOs0mS6HF5XlV9E7gUIMlpwMPAjcOOJUlat92ljyuBb1fVca/3kyTN1nbf8HIN8KlpdyRZApYARqMRXded3GTSFvbt2ze35zp8+PDcnkvaLH2/OCDJmcB/AG+oqv96qW3H43H5zkS1aOHACscOXn2qx5BeJMnRqhpPu287Sx/vAG7fKtKSpNnaTqj3c5xlD0nScHqFOsk5wNuAzw07jiRps17/mFhVTwI/MvAskqQpfGeiJDXOUEtS4wy1JDXOUEtS4wy1JDXOUEtS4wy1JDXOUEtS4wy1JDXOUEtS4wy1JDXOUEtS4wy1JDXOUEtS4wy1JDXOUEtS4wy1JDXOUEtS4wy1JDWu75fbviLJZ5Lcl+TeJD879GCSpDW9vtwW+AhwqKp+OcmZwMsHnEmStMGWoU5yHvBW4L0AVfU08PSwY0mS1vU5o34V8D3gb5NcAhwFrq2qJzdulGQJWAIYjUZ0XTfjUaXZ8NjUTpOqeukNkjFwC3B5Vd2a5CPA41X1h8d7zHg8riNHjsx2UmkGFg6scOzg1ad6DOlFkhytqvG0+/r8Y+JDwENVdevk788Ab5rVcJKkl7ZlqKvqu8B3klw8uelK4J5Bp5IkPafvVR+/C3xicsXH/cBvDDeSJGmjXqGuqjuAqWsnkqRh+c5ESWqcoZakxhlqSWqcoZakxhlqSWqcoZakxhlqSWqcoZakxhlqSWqcoZakxhlqSWqcoZakxhlqSWqcoZakxhlqSWqcoZakxvX9hhdpcJd8+Ms89tQzgz/PwoGVQfd/3tlncOcfvX3Q59DuYqjVjMeeembwbwjvuo7FxcVBn2Po/xFo93HpQ5IaZ6glqXG9lj6SHAOeAL4PPFtVftGtJM3Jdtao91XVo4NNIkmayqUPSWpc3zPqAr6cpICPVdXy5g2SLAFLAKPRiK7rZjakdo+hj5vV1dW5HJse/5qlvqH+uap6OMmPAl9Jcl9V3bRxg0m8lwHG43ENfQmUfgAdWhn80rl5XJ43j9eh3aXX0kdVPTz57yPAjcCbhxxKkvS8LUOd5Jwke9Z/B94O3DX0YJKkNX2WPkbAjUnWt/9kVR0adCpJ0nO2DHVV3Q9cModZJElTeHmeJDXOUEtS4wy1JDXOUEtS4wy1JDXOUEtS4wy1JDXOUEtS4wy1JDXOUEtS4wy1JDXOUEtS4wy1JDXOUEtS4wy1JDXOUEtS4wy1JDXOUEtS4wy1JDWud6iTnJbkn5N8YciBJEkvtJ0z6muBe4caRJI0Xa9QJ7kQuBr462HHkSRtdnrP7f4C+H1gz/E2SLIELAGMRiO6rjvZ2bQLDX3crK6uzuXY9PjXLG0Z6iTvBB6pqqNJFo+3XVUtA8sA4/G4FhePu6k03aEVhj5uuq4b/Dnm8Tq0u/RZ+rgc+MUkx4AbgCuS/MOgU0mSnrNlqKvqD6rqwqpaAK4BvlpVvzr4ZJIkwOuoJal5ff8xEYCq6oBukEkkSVN5Ri1JjTPUktQ4Qy1JjTPUktQ4Qy1JjTPUktQ4Qy1JjTPUktS4bb3hRRrSntcd4I3XHxj+ia4fdvd7XgdrnwoszYahVjOeuPcgxw4OG7h5fHrewoGVQfev3celD0lqnKGWpMYZaklqnKGWpMYZaklqnKGWpMYZaklqnKGWpMYZaklq3JahTnJWktuS3Jnk7iQfnsdgkqQ1fd5C/n/AFVW1muQM4GtJ/qmqbhl4NkkSPUJdVQWsTv48Y/JTQw4lSXperw9lSnIacBT4CeC6qrp1yjZLwBLAaDSi67oZjqndYujjZnV1dS7Hpse/ZqlXqKvq+8ClSV4B3JjkJ6vqrk3bLAPLAOPxuIb+hDL9ADq0Mvgn283j0/Pm8Tq0u2zrqo+q+l/gMHDVINNIkl6kz1Uf50/OpElyNvA24L6B55IkTfRZ+nglcP1knfplwKer6gvDjiVJWtfnqo9/AS6bwyySpCl8Z6IkNc5QS1LjDLUkNc5QS1LjDLUkNc5QS1LjDLUkNc5QS1LjDLUkNc5QS1LjDLUkNc5QS1LjDLUkNc5QS1LjDLUkNc5QS1LjDLUkNc5QS1LjDLUkNa7Pt5BflORwknuS3J3k2nkMJkla0+dbyJ8FPlhVtyfZAxxN8pWqumfg2SRJ9Dijrqr/rKrbJ78/AdwLXDD0YJKkNX3OqJ+TZAG4DLh1yn1LwBLAaDSi67oZjKfdZujjZnV1dS7Hpse/Zql3qJOcC3wW+EBVPb75/qpaBpYBxuNxLS4uzmpG7RaHVhj6uOm6bvDnmMfr0O7S66qPJGewFulPVNXnhh1JkrRRn6s+AvwNcG9V/fnwI0mSNupzRn058GvAFUnumPz8wsBzSZImtlyjrqqvAZnDLJKkKXxnoiQ1bluX50lDWziwMvyTHBr2Oc47+4xB96/dx1CrGccOXj34cywcWJnL80iz5NKHJDXOUEtS4wy1JDXOUEtS4wy1JDXOUEtS4wy1JDXOUEtS4wy1JDXOUEtS4wy1JDXOUEtS4wy1JDXOUEtS4wy1JDXOUEtS4wy1JDVuy1An+XiSR5LcNY+BJEkv1OeM+u+AqwaeQ5J0HFuGuqpuAv57DrNIkqaY2ZfbJlkClgBGoxFd181q19JMeWxqp5lZqKtqGVgGGI/Htbi4OKtdS7NzaAWPTe00XvUhSY0z1JLUuD6X530K+DpwcZKHkrxv+LEkSeu2XKOuqv3zGESSNJ1LH5LUOEMtSY0z1JLUOEMtSY0z1JLUOEMtSY0z1JLUOEMtSY0z1JLUOEMtSY0z1JLUOEMtSY0z1JLUOEMtSY0z1JLUOEMtSY0z1JLUOEMtSY0z1JLUuF6hTnJVkm8m+VaSA0MPJUl6Xp9vIT8NuA54B/B6YH+S1w89mCRpTZ8z6jcD36qq+6vqaeAG4F3DjiVJWnd6j20uAL6z4e+HgJ/ZvFGSJWAJYDQa0XXdLOaTjmvfvn0n9Lj86fYfc/jw4RN6LmkW+oS6l6paBpYBxuNxLS4uzmrX0lRVte3HdF2Hx6Z2mj5LHw8DF234+8LJbZKkOegT6m8Ar0nyqiRnAtcA/zjsWJKkdVsufVTVs0l+B/gScBrw8aq6e/DJJElAzzXqqvoi8MWBZ5EkTeE7EyWpcYZakhpnqCWpcYZakhqXE3nTwJY7Tb4HPDjzHUsnby/w6KkeQprix6rq/Gl3DBJqqVVJjlTV+FTPIW2HSx+S1DhDLUmNM9TabZZP9QDSdrlGLUmN84xakhpnqCWpcYZaO0qSUZJPJrk/ydEkX0/yS0kWkzyW5I4k9yX5s02P25vkmSTv33T7sST/Ovm5J8kfJzlrct9Ckqcm+1z/+fV5vl4JDLV2kCQBPg/cVFWvrqqfZu3z0S+cbHJzVV0KXAa8M8nlGx7+K8AtwP4pu95XVW9k7ftBXw18bMN9366qSzf8/P1MX5TUg6HWTnIF8HRVfXT9hqp6sKr+cuNGVfUUcAdr3/e5bj/wQeCCJBcyRVWtAu8H3p3kh2c8u3TCDLV2kjcAt2+1UZIfAl4D3DT5+yLglVV1G/Bp4D3He2xVPQ48MHk8wI9vWvp4y0m+BmnbDLV2rCTXJbkzyTcmN70lyZ2sfafnl6rqu5Pb38NaoAFuYPryxwt2veH3zUsfN8/sBUg9GWrtJHcDb1r/o6p+G7gSWP8gm5ur6hLWzrzfl+TSye37gfcmOcba933+VJL1M+YXSLIHWAD+bYD5pRNiqLWTfBU4K8lvbbjt5Zs3qqoHgIPAh5K8Fji3qi6oqoWqWgD+hCln1UnOBf4K+HxV/c8QL0A6EYZaO0atvY323cDPJ3kgyW3A9cCHpmz+UeCtrAX5xk33fZYXhvpwkruA24B/B35zw32b16h/bzavRurPt5BLUuM8o5akxhlqSWqcoZakxhlqSWqcoZakxhlqSWqcoZakxv0/KCza4H4HqjcAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAXlklEQVR4nO3df5RcZX3H8fengGhZTgKGbtMQXemJWEg0kpHiwXJmpbYBLPEHpVCERND1B5ziKS1GtIJ6PKVKsBKsNAqHoIGFgpA0gjZGVrQVMIGUhJ8GDEiMu0LCwkIOGvj2j7mjwzKbvTtzd2f26ed1zpy989x7n/t85yafvXvnzlxFBGZmlpbfa/UAzMyseA53M7MEOdzNzBLkcDczS5DD3cwsQQ53M7MEOdzNzBLkcLe2I2mo5vGipJ01z0+RdIGk32TPn5L0P5LeWqefKyXtkjR9WPsFkkLSiTVte2ZtXdnzAyXdIOkJSYOSNklalM3rypatjmmLpMU1fUnSP0r6aTb2xyT9s6S9h43t19n62yWtkfSGrL5qvzuz+n/7ehT/aluqHO7WdiKio/oAHgP+qqZtRbbYtdn8acCtwH/U9iFpH+C9wCDwvjqb2Q58RtIeIwzjG8DPgdcCrwZOBfqHLTM1G8PJwKclzc/aLwF6gNOAfYFjgKOB64at/4Vs/RnAVuDyiFhRU/sxwC+GvR5muTjcbVKLiF3ACmCGpANqZr0XeAr4LLCwzqrfAX5N/eAHeAtwZUQ8GxG7IuLuiLhlhDH8GLgXmC1pFvBR4JSI+HG27r3ZeOZLenud9XdSCf65oxZslpPD3SY1Sa+gcoT8JLCjZtZC4BqgF3iDpHnDVg3gn4DzJe1Vp+vbga9IOknSa3azfUk6EjgUuJvKEfrjEXHnSzYW8fOsz3fU6WMfKkf/m3dXq9lYONxtsjpR0lPATuCDwAnZUTxZGHcDV0dEP7CWyi+Al4iIVcCvgA/U6f+vgR9S+QXwM0kbJL1l2DJPUDm983VgcUSspXKaaNsIY96Wza/6h6yGZ4C3UTn1Y1YIh7tNVtdFxFSgE9gE1B6ZnwrcHxEbsucrgL8d4Qj9U8AngVfWNkbEjohYHBGHZtvYANwkSTWLTYuI/SLiTyLikqztCeAlb+DWmJ7Nr7ooq6GLyi+pg0es1myMHO42qUXEE1TevLyg5qqY04CDJP1S0i+Bi6kcMR9bZ/01VE6HfHSUbVwE/BGw/yhD+j4wU9LhtY2SZgJHUPkrYnj/jwFnA1+W9KpR+jfLxeFuk15EPAh8Fzg3uyTyj4HDqbxBOReYDVxNnVMzmU8C59Y2SPoXSbOzSyT3BT4CbI6IJ0cZy0PAZcAKSUdI2kPSocANwPci4nsjrLcG+AWVX1RmTXO4Wyq+SCUYPwisjIiNEfHL6gP4MvBOSS878o6I/wbuHNb8+8CNVK64eYTKJZHH5xzLWVTOw38TGKJyZU4flStmRqvh3Nrr4c0aJd+sw8wsPT5yNzNLkMPdzCxBDnczswQ53M3MErRnqwcAMG3atOjq6mp4/WeffZZ99tmnuAG1SCp1gGtpR6nUAa6lav369U9ExAH15rVFuHd1dbFu3bqG1+/r66NcLhc3oBZJpQ5wLe0olTrAtVRJenSkeT4tY2aWIIe7mVmCHO5mZglyuJuZJcjhbmaWIIe7mVmCHO5mZglyuJuZJcjhbmaWoLb4hGqzNm4dZNHib0/4drdceNyEb9PMLA8fuZuZJWjUcJc0U9Ktku6TdK+ks7P2/SWtkfTT7Od+WbskXSJps6R7JB023kWYmdlL5Tly3wWcExGHULl7+5mSDgEWA2sjYhaVO7ovzpY/BpiVPXqArxY+ajMz261Rwz0itkXEXdn0M8D9wAxgAbA8W2w58K5segFwVVTcDkyVNL3ogZuZ2cjGdINsSV3AbcBs4LGImJq1C9gREVMlrQYujIgfZfPWAh+PiHXD+uqhcmRPZ2fnvN7e3oaLGNg+SP/Ohldv2JwZUwrtb2hoiI6OjkL7bBXX0n5SqQNcS1V3d/f6iCjVm5f7ahlJHcANwMci4ulKnldEREjK/1uiss4yYBlAqVSKZr6beemKlSzZOPEX/mw5pVxof/6O6vaUSi2p1AGuJY9cV8tI2otKsK+IiG9lzf3V0y3Zz4GsfSsws2b1A7M2MzObIHmulhFwOXB/RFxcM2sVsDCbXgisrGk/Lbtq5ghgMCK2FThmMzMbRZ5zGUcCpwIbJW3I2s4DLgSuk3QG8ChwYjbvZuBYYDPwHPD+IgdsZmajGzXcszdGNcLso+ssH8CZTY7LzMya4E+ompklyOFuZpYgh7uZWYIc7mZmCXK4m5klyOFuZpYgh7uZWYIc7mZmCXK4m5klyOFuZpYgh7uZWYIc7mZmCXK4m5klyOFuZpYgh7uZWYIc7mZmCcpzm70rJA1I2lTTdq2kDdljS/UOTZK6JO2smXfZOI7dzMxGkOc2e1cClwJXVRsi4m+q05KWAIM1yz8cEXMLGp+ZmTUgz232bpPUVW9edvPsE4G3FzwuMzNrgiq3PB1loUq4r46I2cPajwIujohSzXL3Ag8BTwOfiogfjtBnD9AD0NnZOa+3t7fhIga2D9K/s+HVGzZnxpRC+xsaGqKjo6PQPlvFtbSfVOoA11LV3d29vpq/w+U5LbM7JwPX1DzfBrwmIp6UNA+4SdKhEfH08BUjYhmwDKBUKkW5XG54EEtXrGTJxmZLGbstp5QL7a+vr49mXod24lraTyp1gGvJo+GrZSTtCbwHuLbaFhHPR8ST2fR64GHg9c0O0szMxqaZSyH/HHggIh6vNkg6QNIe2fRBwCzgkeaGaGZmY5XnUshrgB8DB0t6XNIZ2ayTeOkpGYCjgHuySyOvBz4cEdsLHK+ZmeWQ52qZk0doX1Sn7QbghuaHZWZmzfAnVM3MEuRwNzNLkMPdzCxBDnczswQ53M3MEuRwNzNLkMPdzCxBDnczswQ53M3MEuRwNzNLkMPdzCxBDnczswQ53M3MEuRwNzNLkMPdzCxBDnczswTluRPTFZIGJG2qabtA0lZJG7LHsTXzPiFps6QHJf3leA3czMxGlufI/Upgfp32L0XE3OxxM4CkQ6jcfu/QbJ1/q95T1czMJs6o4R4RtwF574O6AOiNiOcj4mfAZuDwJsZnZmYNUESMvpDUBayOiNnZ8wuARcDTwDrgnIjYIelS4PaI+Ga23OXALRFxfZ0+e4AegM7Oznm9vb0NFzGwfZD+nQ2v3rA5M6YU2t/Q0BAdHR2F9tkqrqX9pFIHuJaq7u7u9RFRqjdv1Btkj+CrwOeAyH4uAU4fSwcRsQxYBlAqlaJcLjc4FFi6YiVLNjZaSuO2nFIutL++vj6aeR3aiWtpP6nUAa4lj4aulomI/oh4ISJeBL7G7069bAVm1ix6YNZmZmYTqKFwlzS95um7geqVNKuAkyTtLel1wCzgzuaGaGZmYzXquQxJ1wBlYJqkx4HzgbKkuVROy2wBPgQQEfdKug64D9gFnBkRL4zLyM3MbESjhntEnFyn+fLdLP954PPNDMrMzJrjT6iamSXI4W5mliCHu5lZghzuZmYJcribmSXI4W5mliCHu5lZghzuZmYJcribmSXI4W5mliCHu5lZghzuZmYJcribmSXI4W5mliCHu5lZghzuZmYJGjXcJV0haUDSppq2L0p6QNI9km6UNDVr75K0U9KG7HHZOI7dzMxGkOfI/Upg/rC2NcDsiHgj8BDwiZp5D0fE3Ozx4WKGaWZmYzFquEfEbcD2YW3/FRG7sqe3AweOw9jMzKxBRZxzPx24peb56yTdLekHkv6sgP7NzGyMFBGjLyR1AasjYvaw9k8CJeA9ERGS9gY6IuJJSfOAm4BDI+LpOn32AD0AnZ2d83p7exsuYmD7IP07G169YXNmTCm0v6GhITo6Ogrts1VcS/tJpQ5wLVXd3d3rI6JUb96ejQ5I0iLgncDRkf2GiIjngeez6fWSHgZeD6wbvn5ELAOWAZRKpSiXy40OhaUrVrJkY8OlNGzLKeVC++vr66OZ16GduJb2k0od4FryaOi0jKT5wLnA8RHxXE37AZL2yKYPAmYBjxQxUDMzy2/Uw11J1wBlYJqkx4HzqVwdszewRhLA7dmVMUcBn5X0G+BF4MMRsb1ux2ZmNm5GDfeIOLlO8+UjLHsDcEOzgzIzs+b4E6pmZglyuJuZJcjhbmaWIIe7mVmCHO5mZglyuJuZJcjhbmaWIIe7mVmCHO5mZglyuJuZJcjhbmaWIIe7mVmCHO5mZglyuJuZJcjhbmaWIIe7mVmCHO5mZgnKFe6SrpA0IGlTTdv+ktZI+mn2c7+sXZIukbRZ0j2SDhuvwZuZWX15j9yvBOYPa1sMrI2IWcDa7DnAMVRujD0L6AG+2vwwzcxsLHKFe0TcBgy/0fUCYHk2vRx4V037VVFxOzBV0vQCxmpmZjkpIvItKHUBqyNidvb8qYiYmk0L2BERUyWtBi6MiB9l89YCH4+IdcP666FyZE9nZ+e83t7ehosY2D5I/86GV2/YnBlTCu1vaGiIjo6OQvtsFdfSflKpA1xLVXd39/qIKNWbt2dTo8pEREjK91vid+ssA5YBlEqlKJfLDW9/6YqVLNlYSCljsuWUcqH99fX10czr0E5cS/tJpQ5wLXk0c7VMf/V0S/ZzIGvfCsysWe7ArM3MzCZIM+G+CliYTS8EVta0n5ZdNXMEMBgR25rYjpmZjVGucxmSrgHKwDRJjwPnAxcC10k6A3gUODFb/GbgWGAz8Bzw/oLHbGZmo8gV7hFx8gizjq6zbABnNjMoMzNrjj+hamaWIIe7mVmCHO5mZglyuJuZJcjhbmaWIIe7mVmCHO5mZglyuJuZJcjhbmaWIIe7mVmCHO5mZglyuJuZJcjhbmaWIIe7mVmCHO5mZglyuJuZJajhu0pLOhi4tqbpIODTwFTgg8CvsvbzIuLmRrdjZmZj13C4R8SDwFwASXtQuQn2jVRuq/eliLioiAGamdnYFXVa5mjg4Yh4tKD+zMysCarc8rTJTqQrgLsi4lJJFwCLgKeBdcA5EbGjzjo9QA9AZ2fnvN7e3oa3P7B9kP6dDa/esDkzphTa39DQEB0dHYX22Squpf2kUge4lqru7u71EVGqN6/pcJf0CuAXwKER0S+pE3gCCOBzwPSIOH13fZRKpVi3bl3DY1i6YiVLNjZ8hqlhWy48rtD++vr6KJfLhfbZKq6l/aRSB7iWKkkjhnsRp2WOoXLU3g8QEf0R8UJEvAh8DTi8gG2YmdkYFBHuJwPXVJ9Iml4z793ApgK2YWZmY9DUuQxJ+wDvAD5U0/wFSXOpnJbZMmyemZlNgKbCPSKeBV49rO3UpkZkZmZN8ydUzcwS5HA3M0uQw93MLEEOdzOzBDnczcwS5HA3M0uQw93MLEEOdzOzBDnczcwS5HA3M0uQw93MLEEOdzOzBDnczcwS5HA3M0uQw93MLEEOdzOzBDV9V2lJW4BngBeAXRFRkrQ/cC3QReVuTCdGxI5mt2VmZvkUdeTeHRFza+7CvRhYGxGzgLXZczMzmyDjdVpmAbA8m14OvGuctmNmZnUoIprrQPoZsIPKDbH/PSKWSXoqIqZm8wXsqD6vWa8H6AHo7Oyc19vb2/AYBrYP0r+z4dUbNmfGlEL7GxoaoqOjo9A+W8W1tJ9U6gDXUtXd3b2+5ozJSzR9zh14W0RslfQHwBpJD9TOjIiQ9LLfIBGxDFgGUCqVolwuNzyApStWsmRjEaWMzZZTyoX219fXRzOvQztxLe0nlTrAteTR9GmZiNia/RwAbgQOB/olTQfIfg40ux0zM8uvqXCXtI+kfavTwF8Am4BVwMJssYXAyma2Y2ZmY9PsuYxO4MbKaXX2BK6OiO9I+glwnaQzgEeBE5vcjpmZjUFT4R4RjwBvqtP+JHB0M32bmVnj/AlVM7MEOdzNzBI08dcP2qTWtfjbuZY7Z84uFuVcNo8tFx5XWF9m/x/4yN3MLEEOdzOzBDnczcwS5HA3M0uQw93MLEEOdzOzBDnczcwS5HA3M0uQw93MLEEOdzOzBDnczcwS5HA3M0uQw93MLEENh7ukmZJulXSfpHslnZ21XyBpq6QN2ePY4oZrZmZ5NPOVv7uAcyLiruw+quslrcnmfSkiLmp+eGZm1oiGwz0itgHbsulnJN0PzChqYGZm1rhCzrlL6gLeDNyRNZ0l6R5JV0jar4htmJlZfoqI5jqQOoAfAJ+PiG9J6gSeAAL4HDA9Ik6vs14P0APQ2dk5r7e3t+ExDGwfpH9nw6s3bM6MKYX2NzQ0REdHR6F9Fm3j1sFcy3W+ikL3SdGv9VhMhv2SRyp1gGup6u7uXh8RpXrzmgp3SXsBq4HvRsTFdeZ3AasjYvbu+imVSrFu3bqGx7F0xUqWbJz4OwYWfeu3vr4+yuVyoX0WbSy32Styn7TyNnuTYb/kkUodUHwtef9dj4cr5+/TcC2SRgz3Zq6WEXA5cH9tsEuaXrPYu4FNjW7DzMwa08yh1ZHAqcBGSRuytvOAkyXNpXJaZgvwoSa2YWZmDWjmapkfAaoz6+bGh2NmZkXwJ1TNzBLkcDczS5DD3cwsQQ53M7MEOdzNzBLkcDczS5DD3cwsQQ53M7MEOdzNzBLkcDczS5DD3cwsQRP/PblmlkvRX0N7zpxdLMrZZyu/YtmK4SN3M7MEOdzNzBLkcDczS5DD3cwsQQ53M7MEjVu4S5ov6UFJmyUtHq/tmJnZy41LuEvaA/gKcAxwCJX7qh4yHtsyM7OXG68j98OBzRHxSET8GugFFozTtszMbBhFRPGdSicA8yPiA9nzU4E/jYizapbpAXqypwcDDzaxyWnAE02s3y5SqQNcSztKpQ5wLVWvjYgD6s1o2SdUI2IZsKyIviSti4hSEX21Uip1gGtpR6nUAa4lj/E6LbMVmFnz/MCszczMJsB4hftPgFmSXifpFcBJwKpx2paZmQ0zLqdlImKXpLOA7wJ7AFdExL3jsa1MIad32kAqdYBraUep1AGuZVTj8oaqmZm1lj+hamaWIIe7mVmCJk24S7pC0oCkTSPMl6RLsq87uEfSYRM9xjxy1FGWNChpQ/b49ESPMS9JMyXdKuk+SfdKOrvOMm2/X3LWMSn2i6RXSrpT0v9mtXymzjJ7S7o22yd3SOpqwVBHlbOWRZJ+VbNfPtCKseYhaQ9Jd0taXWde8fskIibFAzgKOAzYNML8Y4FbAAFHAHe0eswN1lEGVrd6nDlrmQ4clk3vCzwEHDLZ9kvOOibFfsle545sei/gDuCIYct8FLgsmz4JuLbV426ilkXApa0ea856/h64ut6/o/HYJ5PmyD0ibgO272aRBcBVUXE7MFXS9IkZXX456pg0ImJbRNyVTT8D3A/MGLZY2++XnHVMCtnrPJQ93St7DL9qYgGwPJu+HjhakiZoiLnlrGVSkHQgcBzw9REWKXyfTJpwz2EG8POa548zSf+DAm/N/hS9RdKhrR5MHtmfkW+mcnRVa1Ltl93UAZNkv2R//m8ABoA1ETHiPomIXcAg8OoJHWROOWoBeG92yu96STPrzG8H/wqcC7w4wvzC90lK4Z6Ku6h8X8SbgKXATa0dzugkdQA3AB+LiKdbPZ5GjVLHpNkvEfFCRMyl8snwwyXNbvGQGpajlv8EuiLijcAafnf02zYkvRMYiIj1E7ndlMI9ia88iIinq3+KRsTNwF6SprV4WCOStBeVQFwREd+qs8ik2C+j1THZ9gtARDwF3ArMHzbrt/tE0p7AFODJCR3cGI1US0Q8GRHPZ0+/Dsyb4KHlcSRwvKQtVL4h9+2SvjlsmcL3SUrhvgo4Lbs64whgMCK2tXpQYyXpD6vn2iQdTmUfteV/vGyclwP3R8TFIyzW9vslTx2TZb9IOkDS1Gz6VcA7gAeGLbYKWJhNnwB8P7J38tpJnlqGvX9zPJX3S9pKRHwiIg6MiC4qb5Z+PyLeN2yxwvdJy74VcqwkXUPlioVpkh4HzqfyBgsRcRlwM5UrMzYDzwHvb81Idy9HHScAH5G0C9gJnNSO//EyRwKnAhuz86IA5wGvgUm1X/LUMVn2y3RguSo3zPk94LqIWC3ps8C6iFhF5RfZNyRtpvLm/kmtG+5u5anl7yQdD+yiUsuilo12jMZ7n/jrB8zMEpTSaRkzM8s43M3MEuRwNzNLkMPdzCxBDnczswQ53M3MEuRwNzNL0P8B4FA7K1mYQ9IAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function flush_figures at 0x121845ee0> (for post_execute):\n"
     ]
    }
   ],
   "source": [
    "boxplot = df.boxplot(column = 'GRADE')\n",
    "histogram = df.hist(column = 'GRADE')\n",
    "histogram = df.hist(column = 'TRANSPORT')\n",
    "scatterPlot = df.plot.scatter(x='GENDER', y='GRADE', c='orange')\n",
    "scatterPlot.xaxis.label.set_color('white')        #setting up X-axis label color to white\n",
    "scatterPlot.yaxis.label.set_color('white')          #setting up Y-axis label color to white\n",
    "scatterPlot.tick_params(axis='x', colors='white')    #setting up X-axis tick color to white\n",
    "scatterPlot.tick_params(axis='y', colors='white')  #setting up Y-axis tick color to white\n",
    "scatterPlot.spines['left'].set_color('white')        # setting up Y-axis tick color to white\n",
    "scatterPlot.spines['top'].set_color('white')         #setting up above X-axis tick color to white\n",
    "scatterPlot\n",
    "sns.heatmap(df, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampled Data shape: \n",
      "(280, 31)\n",
      "\n",
      "Oversampled Data size: \n",
      "8680\n",
      "\n",
      "Oversampled Data ndim: \n",
      "2\n",
      "\n",
      "_____________________________________________\n",
      "\n",
      "New Class Distribution: Counter({1: 35, 2: 35, 5: 35, 0: 35, 3: 35, 4: 35, 7: 35, 6: 35})\n",
      "_____________________________________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "oversample = SMOTE()\n",
    "x_over, y_over = oversample.fit_resample(df.drop([\"GRADE\"], axis=1), df.drop(df.columns[0:-1],axis=1))\n",
    "df = pd.concat([x_over, y_over], axis=1)\n",
    "\n",
    "# print the dimensionality of the oversampled training dataset (0.5)\n",
    "print(f\"Oversampled Data shape: \\n{df.shape}\\n\")\n",
    "print(f\"Oversampled Data size: \\n{df.size}\\n\")\n",
    "print(f\"Oversampled Data ndim: \\n{df.ndim}\\n\")\n",
    "print(\"_____________________________________________\\n\")\n",
    "\n",
    "\n",
    "# print the new class distribution using the Counter (1)\n",
    "print(f\"New Class Distribution: {Counter(df['GRADE'])}\")\n",
    "print(\"_____________________________________________\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='GENDER', ylabel='GRADE'>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEGCAYAAABvtY4XAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAARwElEQVR4nO3dfbAdZWHH8e8mNzQhAYIQUwooCQNUUAjhFuKAGCBjiVISCs7AxbZDnd6+QMtLrThOW6y2nToOylSpmgEERw+oWCilqJghFBhNNBdTw0sFmvBaGkBeIpBIKds/ns3cw73nnD177u69e577/cyc2XOe3bP77MD95TnPPvtskqYpkqT4zJjqCkiSqmHAS1KkDHhJipQBL0mRMuAlKVIDU12BMRzSI0kFrF+//rlly5YtaLUuqdkwyVpVRpLqLkmSkTRNB1uts4tGkiJlwEtSpAx4SYqUAS9JkTLgJSlSVQb8YcCmptd24KIKjydJ/acxBxpJWJZssoZJzgSeAo4DHuuwncMkJU0fjWR82VCxGKzDMMlTgP+ic7hL0vTRrsVeYkt+su5kPRu4vs264ewlSdPIzoLlxU1GC3434HTgW23WrwEGs5ckTROzC5YXNxkBvxK4F9g2CceSpP4wtKNYeQ8mI+DPoX33jCRNX0Mpoy322YUvsOapehTNXOBxYDHwUhfbO4pGkgroNIqm6ousrwD7VHwMSVIL3skqSZEy4CUpUga8JEXKgJekSBnwkhQpA16SImXAS1KkDHhJipQBL0mRMuAlKVIGvCRFyoCXpEgZ8JIUKQNekiJlwEtSpAx4SYqUAS9JkTLgJSlSBrwkRarqgJ8P3Aj8J/Ag8O5KjrJuNdwwLywlqZ/ctBgaSViWLEnTtPSdNrkOuBu4CtgN2B14scP2xSvTSMaXDVV6TpJUjhLyK0mSkTRNB1utq7IFvxdwInB19vk1Ood7ce1a7LbkJdVduxZ7iS35KgN+EfAs8BXgJ4RW/NwW2w0DG7NXMdvWFiuXpLrYsbVYeQ+qDPgBYCnwReBo4BXgYy22WwMMZq9iFq4oVi5JdTFnUbHyHlQZ8E9mrw3Z5xsJgV+ek24uVi5JdXHGlmLlPagy4P8HeAI4LPt8CvBA6UcZSmG/VTBjblh6gVVSvxhKR1vscxaVnl9Vj6JZwugImi3AecALHbY3nSWpgE6jaAYqPvYmeulblyRNmHeySlKkDHhJipQBL0mRMuAlKVIGvCRFyoCXpEgZ8JIUKQNekiJlwEtSpAx4SYqUAS9JkTLgJSlSBrwkRcqAl6RIGfCSFCkDXpIiZcBLUqQMeEmKlAEvSZEy4CUpUlUH/KPAZsLDtzdWdpRbj4HGzLCUpH5y2zJoDIRlyZI0TUvfaZNHgUHguS63L16ZRjK+bKjSc5KkcpSQX0mSjKRpOthqXX930bRrsduSl1R37VrsJbbkqw74FLgdGAGG22wzTOi+Kd6Fs31TsXJJqosX20Reu/IeVB3wJwBLgZXA+cCJLbZZQ+jGafkTo6M9lxQrl6S6mN8m8tqV96DqgH8qWz4D3AQcW+reTxspVi5JdfH+9cXKe1BlwM8F9mh6/z7gvtKPMpTCnkuBGWHpBVZJ/WIohfnHATPDsuT8qnIUzWJCqx1gAGgAf5fzHdNZkgroNIpmoMLjbgGOqnD/kqQO+nuYpCSpLQNekiJlwEtSpAx4SYqUAS9JkTLgJSlSBrwkRcqAl6RIGfCSFCkDXpIiZcBLUqQMeEmKlAEvSZEy4CUpUga8JEXKgJekSBnwkhQpA16SImXAS1KkJiPgZwI/AW6t7Ag3HwqNJCwlqZ+sXQnXzwnLkiVpmpa+0zEuAQaBPYHTcrYtXplGMr5sqPJzkqSJKyG/kiQZSdN0sNW6vBb8h5reHz9m3QVdHPsA4APAVV1sW1y7FrsteUl1167FXmJLPi/gL2l6//kx636/i/1fAXwUeKPDNsPAxuxVzKsPFyuXpLp49s5i5T3IC/ikzftWn8c6DXgGGMnZbg2hC6flT4yOdj+kWLkk1cWC5cXKe5AX8Gmb960+j3U8cDrwKHADcDLwtSKVy7X6oWLlklQXK75TrLwHeRdZXwUeIbTWD87ek31eDMzt8jjLgY9QxUVWCH3urz4cWu6Gu6R+snZl6JZZsLyncO90kXUg57vvKHy0qWCoS+pXJbbYx+p2mOR8YFfH9kPASxXVx/GNklTARFrwvwJ8GVgNbCV0zbwduAn4I+C18qopSSpT3kXWvwRmAQcCRwNLgLcR/mH4q0prJkmakLwumvuAYwkXW5vNA9YD7yy5PnbRSFIBE7mT9Q3GhzvAyxjGklRreX3wKbA3rW9q6nR3qiRpiuUF/F6EO1Hz7lqVJNVMXsAfNBmVkCSVr5f54A8mjKC5v+S6SJJK1G3A/xpwMfBjQrDPAM6uqlKSpInLC/hhYB1wJ7AP8GHgaeBvgM2V1kySNCF5ffBfAH4IDDE6X7vDIyWpD+QF/H7AB4HLgV8Fvkm4s1WSVHN5XTQ/B74EvBc4BXgR2AY8CPx9pTWTJE1IkVE0TxJa8oOEB3nsqKRGkqRSdBPw7wbOAt6afT6ScJF1uKpKSZImLi/gPwNcA5wJ/Bvwt8DtwAZG54eXJNVQ3kXWDxCmCd5JmJPmCcIMko9WWy1J0kTlteB3Zi+AF4CHMdwlqS/kteAXA7c0fV6UfU4I4+FPr6hekqQJygv4VWM+X87ojU55M0zOBu4iPPZvALgRuKxoBbuybjVsWwsLV8BJN1dyCEmqxE2LYcdWmLMIzthS6q7znui0CjgAuDL7/CNgASHkLwW+1WnfwFzCw0FmAfcAFxKeBNVO8btkGy3+nRnyZltJfaCE/JrIE50+ypu7aHYjjINfTnjodicpIdwhBPwsyp7mYN3qYuWSVBc3LS5W3oO8gN+NMHJml3sId7c+Tmid55kJbAKeAb5PGF451jBhnpuNLdZ1tm1tsXJJqosdW4uV9yAv4Pce8/mCpvcLutj//wFLCN08x9L6Id1rCL8KWv7E6GjhimLlklQXcxYVK+9BXsBvAP6gRfkfEvrju/UiYdrhUwt8J1+7C6peaJVUd+0uqJZ4oTXvIutbgZuBXwL3ZmXHEEbGrCZMPNbOAuB/CeE+h3AH7KeBWzt8p7c+ekfRSOpXExxF0+kia17A73IycET2/n7gji6+cyRwHaEffgZhquFP5nzH4S+SVEAZAT9ZalUZSaq7iQyTlCT1KQNekiJlwEtSpAx4SYqUAS9JkTLgJSlSBrwkRcqAl6RIGfCSFCkDXpIiZcBLUqQMeEmKlAEvSZEy4CUpUga8JEXKgJekSBnwkhQpA16SImXAS1Kkqgz4A4F1wAOEB3VfWNmRbj0GGjPDUpL6yW3LoDEQliWr8qHb+2Wve4E9gBFgNSHw2ylemUYyvmzIZ3dL6gMl5NdUPXT7aUK4A/wCeBDYv9QjtGux25KXVHftWuwltuQHSttTZwcBRwMbWqwbzl7Fbd9UrFyS6uLFjcXKezAZF1nnAd8GLgK2t1i/BhjMXsXsuaRYuSTVxfw2kdeuvAdV9sEDzAJuBb4HfLaL7e2DlzR99HEffAJcTeh77ybcezOUwp5LgRlhabhL6hdDKcw/DpgZliXnV5Ut+BOAu4HNwBtZ2ceB2zp8x3SWpAI6teCrvMh6D6EVL0maAt7JKkmRMuAlKVIGvCRFyoCXpEgZ8JIUKQNekiJlwEtSpAx4SYqUAS9JkTLgJSlSBrwkRcqAl6RIGfCSFCkDXpIiZcBLUqQMeEmKlAEvSZEy4CUpUga8JEXKgJekSFUZ8NcAzwD3VXiM4LvvgcassJSkfnLLkdCYEZYlS9I0LX2nmROBl4GvAu/s8jvFK9NIxpcNVXZOklSeEvIrSZKRNE0HW62rsgV/F/B8hftv32K3JS+p7tq12EtsyQ+UtqfeDWev4p5fX6xckuri5Ta91+3Ke1CHi6xrgMHsVcxblhUrl6S6mNem57pdeQ/qEPC9O/XuYuWSVBen/7RYeQ/6O+AhXJB4ywnAQFh6gVVSvxhKYd67gCQsS86vKkfRXA8sB/YFtgGXAVfnfMd0lqQCOo2iqfIi6zkV7luSlKP/u2gkSS0Z8JIUKQNekiJlwEtSpAx4SYqUAS9JkTLgJSlSBrwkRcqAl6RIGfCSFCkDXpIiZcBLUqQMeEmKlAEvSZEy4CUpUga8JEXKgJekSBnwkhQpA16SIlV1wJ8K/Ax4BPhYZUdpzIFGEpaSJKDagJ8JXAmsBA4nPIT78NKP0kiAndmHndlnSVKVAX8soeW+BXgNuAFYVeoR2rXYbclLUqUBvz/wRNPnJ7OysYaBjdmroJ0FyyVp+qjDRdY1wGD2Kmh2wXJJmj6qDPingAObPh+QlZVnaEexckmaRqoM+B8DhwCLgN2As4FbSj/KUMpoi3129lmSNFDhvl8HLgC+RxhRcw1wfyVHssUuSeMkaVqrFm+tKiNJdZckyUiapi2vYdbhIqskqQIGvCRFyoCXpEgZ8JIUqSpH0RS2bNmy5zZs2PBYL99duHDhvtu2bXuu7DrVmeccv+l2vuA59+Dt7VbUbRTNRGykp7th+5rnHL/pdr7gOZfGLhpJipQBL0mRiing10x1BaaA5xy/6Xa+4DmXJqY+eElSk5ha8JKkJga8JEWq3wL+GuAZ4L426xPgHwmPCvwpsHSS6lWlvHM+l3Cum4EfAEdNUr2qlHfOu/wGYdbSsyqvUfW6OeflwCbCrKz/Xn2VKpV3vnsB/wr8B+F8z5ukelXpQGAd8ADhnC5ssU2pGdZvAX8tcGqH9SsJc9AfQngU4BcnoU5Vu5bO57wVeC/wLuBTxHGB6lo6nzOEKag/DdxeeW0mx7V0Puf5wD8BpwNHAB+svkqVupbO53s+IQiPIvzDdjnhuRL97HXgz4HDgWWEczx8zDalZli/BfxdwPMd1q8CvkqYdng94Y9iv+qrVam8c/4B8EL2fj3hyVn9Lu+cAf4U+DahFRiDvHMeAv4ZeDz73O/nnXe+KbAHoUU7L9v29UmoV5WeBu7N3v8CeJDxz6kuNcP6LeDzdPug71h9GPjOVFdiEuwPnEEcv9C6dSiwN3AnMAL87pTWpnpfAN4B/Deh+/FC4I0prVG5DgKOBjaMKS81w2o1F40m5CRCwJ8w1RWZBFcAlxLXH3yeAeAY4BRgDvBDQgvvoamsVIV+k3C94WTgYOD7wN3A9imsU1nmEX59XkTF5xNbwFf/oO96OhK4itB/9/MprstkGARuyN7vC7yf8PP95qmq0CR4kvDf9pXsdRehfzrWgD8P+AdCV8UjhGtNvw78aCorVYJZhHD/OqHLbaxSMyy2LppbCD9dE8JFjJcI/V4xexvhf5TfId4/9rEWEX7iHgTcCPwJcYc7wL8Qfp0NALsDxxH6cGP1OOHXCsBC4DBgy9RVpxQJcDXhv9tn22xTaob1Wwv+esIV9X0JLZrLCP8iAnwJuI3QmnsEeJU4hlblnfNfA/sQRlhAaMn2+0x8eecco7xzfhD4LmHo3BuEX2x5w0jrLO98P0UYabOZEHaXAv0+hfDxhIbYZkL3E8DHCY00qCDDnKpAkiIVWxeNJCljwEtSpAx4SYqUAS9JkTLgJSlSBrxitRBoEMZOjxDu/DyDMDTvJcIwtV2vFdl3UsKkVrt8BPhE9v4ThBtONgEPE+49aJ4o6k7gZ037vLHF9x4AzpnAOUmFGPCKUUK48ekuYDHhFv+zGZ2I7W5gSdNrbVb+S+C3CWOzW/lctv0hwDeAO4AFTevPbdrnWS2+twr4MqPjvaVKGfCK0cnAa7z5pqjHgM/nfO91wnTLF3dxjG8QpioeKlCvhwk3r+xd4DtSzwx4xegIRqdlbeU9vLmL5uCmdVcSWuJ7dXGcewnzo+zy9aZ9fqbF9ksJId/vU/2qT/TbVAVSL64kzOPyGvAXhC6a09psu50wH/efATty9puM+XwusLHFdhcTbjk/FPit7qosTZwteMXoft78qLPzCRNXLWi9+ThXEKZenpuz3dF0N+HX5wi/Ks4kTDY1u8t6SBNiwCtGdxBC9I+bynYv8P3ngW8SQr6dM4H3ESbN6tYthBb+7xX4jtQzA14xSoHVhGfVbiXMIX4dYUZCGN8H3+qh3ZczfjTNxYwOk/wQ4WLus03rm/vg19LaJ4FL8G9Pk8DZJCUpUrYiJClSBrwkRcqAl6RIGfCSFCkDXpIiZcBLUqQMeEmK1P8DUZi+UhZ1wKcAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "dark"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#how oversampling changes our data\n",
    "boxplot = df.boxplot(column = 'GRADE')\n",
    "histogram = df.hist(column = 'GRADE')\n",
    "histogram = df.hist(column = 'TRANSPORT')\n",
    "scatterPlot = df.plot.scatter(x='GENDER', y='GRADE', c='orange')\n",
    "scatterPlot.xaxis.label.set_color('white')        #setting up X-axis label color to white\n",
    "scatterPlot.yaxis.label.set_color('white')          #setting up Y-axis label color to white\n",
    "scatterPlot.tick_params(axis='x', colors='white')    #setting up X-axis tick color to white\n",
    "scatterPlot.tick_params(axis='y', colors='white')  #setting up Y-axis tick color to white\n",
    "scatterPlot.spines['left'].set_color('white')        # setting up Y-axis tick color to white\n",
    "scatterPlot.spines['top'].set_color('white')         #setting up above X-axis tick color to white\n",
    "scatterPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data shape: \n",
      "     AGE  GENDER  HS_TYPE  SCHOLARSHIP  WORK  ACTIVITY  PARTNER  SALARY  \\\n",
      "86     2       2        2            4     2         2        2       1   \n",
      "137    1       1        1            5     2         1        2       1   \n",
      "184    2       2        2            4     1         2        2       1   \n",
      "5      2       2        2            3     2         2        2       2   \n",
      "124    1       1        2            4     1         1        1       1   \n",
      "..   ...     ...      ...          ...   ...       ...      ...     ...   \n",
      "188    1       2        2            3     1         2        1       4   \n",
      "71     1       1        3            4     2         2        2       1   \n",
      "106    1       2        2            4     2         1        2       1   \n",
      "270    1       2        2            4     2         1        1       1   \n",
      "102    1       2        2            3     2         2        1       1   \n",
      "\n",
      "     TRANSPORT  LIVING  ...  ATTEND  PREP_STUDY  PREP_EXAM  NOTES  LISTENS  \\\n",
      "86           4       2  ...       1           1          1      3        2   \n",
      "137          1       1  ...       1           1          1      3        1   \n",
      "184          1       1  ...       1           1          1      3        2   \n",
      "5            1       1  ...       1           1          1      1        2   \n",
      "124          1       3  ...       1           2          2      3        3   \n",
      "..         ...     ...  ...     ...         ...        ...    ...      ...   \n",
      "188          1       1  ...       1           1          1      1        2   \n",
      "71           1       3  ...       1           1          1      2        3   \n",
      "106          1       2  ...       1           3          2      2        2   \n",
      "270          1       2  ...       1           1          1      3        2   \n",
      "102          1       2  ...       1           1          1      3        1   \n",
      "\n",
      "     LIKES_DISCUSS  CLASSROOM  CUML_GPA  EXP_GPA  GRADE  \n",
      "86               3          2         5        4      5  \n",
      "137              3          1         2        4      2  \n",
      "184              2          1         4        3      3  \n",
      "5                1          2         4        4      2  \n",
      "124              2          1         3        3      3  \n",
      "..             ...        ...       ...      ...    ...  \n",
      "188              2          1         4        3      3  \n",
      "71               3          3         2        2      6  \n",
      "106              2          1         4        4      7  \n",
      "270              3          2         2        3      7  \n",
      "102              2          1         3        4      7  \n",
      "\n",
      "[224 rows x 31 columns]\n",
      "\n",
      "Test Data shape: \n",
      "     AGE  GENDER  HS_TYPE  SCHOLARSHIP  WORK  ACTIVITY  PARTNER  SALARY  \\\n",
      "33     2       1        2            3     1         2        1       1   \n",
      "108    2       1        1            5     2         1        2       2   \n",
      "240    1       2        2            3     1         1        2       1   \n",
      "259    1       1        2            4     1         1        2       1   \n",
      "154    1       1        1            3     1         1        1       1   \n",
      "9      2       1        2            3     2         2        1       3   \n",
      "146    1       1        1            3     1         1        1       1   \n",
      "203    1       2        2            4     1         1        2       1   \n",
      "144    1       1        1            5     2         2        2       3   \n",
      "155    1       1        1            4     2         1        1       1   \n",
      "221    1       1        2            1     1         2        1       1   \n",
      "92     1       2        2            3     2         2        2       1   \n",
      "222    2       1        2            3     1         1        1       1   \n",
      "209    1       2        2            2     1         2        1       1   \n",
      "42     2       2        2            3     2         1        2       1   \n",
      "210    2       1        2            3     1         1        1       1   \n",
      "66     2       2        2            3     2         2        1       1   \n",
      "90     2       1        2            3     2         1        1       1   \n",
      "119    2       1        2            4     2         1        2       1   \n",
      "142    1       1        1            4     2         2        2       1   \n",
      "262    1       2        2            3     1         2        1       1   \n",
      "268    1       2        2            3     2         1        1       1   \n",
      "206    2       1        2            3     1         1        2       1   \n",
      "238    2       1        2            4     1         1        1       1   \n",
      "46     2       2        2            3     2         2        1       1   \n",
      "77     1       2        1            2     2         2        1       2   \n",
      "68     2       1        2            4     1         2        2       1   \n",
      "75     1       2        2            4     2         1        2       1   \n",
      "216    2       2        1            2     1         1        2       1   \n",
      "277    1       2        1            2     2         2        1       2   \n",
      "45     1       2        2            3     2         2        1       4   \n",
      "111    1       1        1            5     2         1        2       1   \n",
      "60     2       1        2            3     2         2        2       5   \n",
      "217    2       1        2            3     1         1        1       1   \n",
      "143    2       1        2            4     1         1        1       5   \n",
      "30     2       2        2            5     1         1        1       1   \n",
      "22     2       2        2            3     1         2        1       1   \n",
      "24     2       2        2            3     2         2        2       2   \n",
      "127    1       1        2            4     2         2        2       1   \n",
      "176    1       2        2            3     1         2        1       3   \n",
      "79     2       2        2            4     2         2        2       1   \n",
      "264    1       2        1            3     2         1        1       1   \n",
      "237    2       1        2            4     2         2        1       1   \n",
      "120    2       1        1            3     1         1        1       2   \n",
      "196    1       1        1            3     1         1        1       1   \n",
      "245    1       1        2            3     1         1        1       1   \n",
      "168    1       1        1            4     2         1        1       1   \n",
      "6      1       2        2            4     2         2        2       1   \n",
      "239    2       1        2            3     1         2        1       1   \n",
      "73     2       2        2            4     2         2        2       1   \n",
      "84     3       2        3            3     1         2        1       3   \n",
      "56     2       2        2            3     2         1        2       1   \n",
      "25     2       2        2            3     2         2        1       1   \n",
      "97     1       2        2            4     1         2        2       1   \n",
      "147    1       1        1            4     1         2        1       1   \n",
      "19     1       2        1            3     2         2        1       2   \n",
      "\n",
      "     TRANSPORT  LIVING  ...  ATTEND  PREP_STUDY  PREP_EXAM  NOTES  LISTENS  \\\n",
      "33           1       1  ...       1           1          1      1        3   \n",
      "108          2       1  ...       2           1          1      2        3   \n",
      "240          1       2  ...       1           1          1      2        2   \n",
      "259          1       3  ...       1           1          1      2        3   \n",
      "154          1       1  ...       1           1          1      2        2   \n",
      "9            4       2  ...       2           1          1      2        2   \n",
      "146          1       2  ...       1           2          1      2        2   \n",
      "203          1       1  ...       1           1          1      3        2   \n",
      "144          1       1  ...       1           2          1      3        2   \n",
      "155          2       1  ...       1           1          1      2        2   \n",
      "221          1       1  ...       1           1          1      2        3   \n",
      "92           1       1  ...       1           1          1      3        2   \n",
      "222          1       2  ...       1           1          1      2        1   \n",
      "209          1       1  ...       1           1          1      3        1   \n",
      "42           4       2  ...       1           1          1      2        1   \n",
      "210          1       1  ...       1           1          1      2        3   \n",
      "66           1       1  ...       1           1          1      3        2   \n",
      "90           2       3  ...       1           1          1      2        3   \n",
      "119          1       2  ...       1           1          2      2        2   \n",
      "142          1       1  ...       1           1          1      3        3   \n",
      "262          1       1  ...       1           1          1      3        2   \n",
      "268          1       2  ...       1           1          1      2        1   \n",
      "206          1       1  ...       1           1          1      3        2   \n",
      "238          1       1  ...       1           1          1      2        2   \n",
      "46           1       1  ...       1           1          1      2        2   \n",
      "77           2       2  ...       2           3          1      2        2   \n",
      "68           1       1  ...       2           1          1      2        2   \n",
      "75           1       3  ...       1           1          1      2        2   \n",
      "216          1       1  ...       1           1          1      2        1   \n",
      "277          1       2  ...       1           2          1      2        2   \n",
      "45           1       1  ...       1           1          1      2        2   \n",
      "111          1       2  ...       1           1          1      3        1   \n",
      "60           2       1  ...       1           1          1      1        3   \n",
      "217          1       1  ...       1           1          1      2        3   \n",
      "143          2       3  ...       1           2          1      2        1   \n",
      "30           1       2  ...       1           2          1      2        3   \n",
      "22           1       1  ...       1           1          2      3        1   \n",
      "24           1       1  ...       1           1          1      2        1   \n",
      "127          4       3  ...       1           1          1      3        2   \n",
      "176          1       1  ...       2           2          1      2        1   \n",
      "79           1       1  ...       1           1          1      3        3   \n",
      "264          1       1  ...       1           1          1      2        2   \n",
      "237          1       1  ...       1           2          1      2        2   \n",
      "120          2       3  ...       2           2          1      3        3   \n",
      "196          1       1  ...       1           1          1      3        2   \n",
      "245          1       2  ...       1           1          1      2        2   \n",
      "168          3       2  ...       1           1          1      2        2   \n",
      "6            1       3  ...       2           1          1      3        3   \n",
      "239          1       1  ...       1           1          1      2        2   \n",
      "73           1       2  ...       1           1          2      3        2   \n",
      "84           1       2  ...       1           3          3      3        3   \n",
      "56           1       1  ...       1           1          2      3        2   \n",
      "25           1       2  ...       1           1          1      2        1   \n",
      "97           1       3  ...       1           1          1      3        2   \n",
      "147          1       1  ...       1           1          1      2        2   \n",
      "19           2       2  ...       1           1          1      3        2   \n",
      "\n",
      "     LIKES_DISCUSS  CLASSROOM  CUML_GPA  EXP_GPA  GRADE  \n",
      "33               2          2         2        3      2  \n",
      "108              1          2         3        3      6  \n",
      "240              2          1         2        2      6  \n",
      "259              2          2         2        2      6  \n",
      "154              2          2         3        2      0  \n",
      "9                2          2         1        2      0  \n",
      "146              2          1         2        2      0  \n",
      "203              3          2         4        3      4  \n",
      "144              3          1         5        4      3  \n",
      "155              2          2         3        2      0  \n",
      "221              2          2         2        2      4  \n",
      "92               3          3         2        2      7  \n",
      "222              2          1         3        3      5  \n",
      "209              3          2         2        2      4  \n",
      "42               3          1         2        3      1  \n",
      "210              1          1         3        2      4  \n",
      "66               2          3         5        4      5  \n",
      "90               2          3         4        2      6  \n",
      "119              3          1         3        3      2  \n",
      "142              2          1         4        3      1  \n",
      "262              2          3         2        2      7  \n",
      "268              2          1         3        4      7  \n",
      "206              2          1         2        2      4  \n",
      "238              3          2         4        3      5  \n",
      "46               3          1         4        2      5  \n",
      "77               2          2         1        1      7  \n",
      "68               3          2         4        3      5  \n",
      "75               2          2         4        1      7  \n",
      "216              2          1         4        3      4  \n",
      "277              2          1         1        1      7  \n",
      "45               2          1         4        3      3  \n",
      "111              3          3         4        3      2  \n",
      "60               3          1         2        1      2  \n",
      "217              1          1         3        2      4  \n",
      "143              2          1         5        3      4  \n",
      "30               3          3         5        4      5  \n",
      "22               2          3         3        3      3  \n",
      "24               3          2         4        4      2  \n",
      "127              2          1         2        2      1  \n",
      "176              2          1         2        2      2  \n",
      "79               3          2         4        4      3  \n",
      "264              3          1         4        4      7  \n",
      "237              2          1         3        2      5  \n",
      "120              3          2         2        2      1  \n",
      "196              2          2         2        3      3  \n",
      "245              2          1         3        2      6  \n",
      "168              2          1         2        2      0  \n",
      "6                3          3         4        4      5  \n",
      "239              2          2         4        3      5  \n",
      "73               3          1         5        3      6  \n",
      "84               3          3         5        4      7  \n",
      "56               3          3         5        4      5  \n",
      "25               3          2         1        2      3  \n",
      "97               2          1         3        3      6  \n",
      "147              2          2         3        2      0  \n",
      "19               2          3         2        3      3  \n",
      "\n",
      "[56 rows x 31 columns]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_train, data_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "print(f\"Train Data shape: \\n{data_train}\\n\")\n",
    "print(f\"Test Data shape: \\n{data_test}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGE</th>\n",
       "      <th>GENDER</th>\n",
       "      <th>HS_TYPE</th>\n",
       "      <th>SCHOLARSHIP</th>\n",
       "      <th>WORK</th>\n",
       "      <th>ACTIVITY</th>\n",
       "      <th>PARTNER</th>\n",
       "      <th>SALARY</th>\n",
       "      <th>TRANSPORT</th>\n",
       "      <th>LIVING</th>\n",
       "      <th>...</th>\n",
       "      <th>ATTEND</th>\n",
       "      <th>PREP_STUDY</th>\n",
       "      <th>PREP_EXAM</th>\n",
       "      <th>NOTES</th>\n",
       "      <th>LISTENS</th>\n",
       "      <th>LIKES_DISCUSS</th>\n",
       "      <th>CLASSROOM</th>\n",
       "      <th>CUML_GPA</th>\n",
       "      <th>EXP_GPA</th>\n",
       "      <th>GRADE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>280 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     AGE  GENDER  HS_TYPE  SCHOLARSHIP  WORK  ACTIVITY  PARTNER  SALARY  \\\n",
       "0      2       2        3            3     1         2        2       1   \n",
       "1      2       2        3            3     1         2        2       1   \n",
       "2      2       2        2            3     2         2        2       2   \n",
       "3      1       1        1            3     1         2        1       2   \n",
       "4      2       2        1            3     2         2        1       3   \n",
       "..   ...     ...      ...          ...   ...       ...      ...     ...   \n",
       "275    1       2        2            4     1         2        1       1   \n",
       "276    1       2        2            4     2         1        1       1   \n",
       "277    1       2        1            2     2         2        1       2   \n",
       "278    1       2        2            4     2         1        1       1   \n",
       "279    1       2        2            4     2         1        2       1   \n",
       "\n",
       "     TRANSPORT  LIVING  ...  ATTEND  PREP_STUDY  PREP_EXAM  NOTES  LISTENS  \\\n",
       "0            1       1  ...       1           1          1      3        2   \n",
       "1            1       1  ...       1           1          1      3        2   \n",
       "2            4       2  ...       1           1          1      2        2   \n",
       "3            1       2  ...       1           1          2      3        2   \n",
       "4            1       4  ...       1           2          1      2        2   \n",
       "..         ...     ...  ...     ...         ...        ...    ...      ...   \n",
       "275          1       1  ...       1           1          1      3        2   \n",
       "276          1       2  ...       1           1          1      3        2   \n",
       "277          1       2  ...       1           2          1      2        2   \n",
       "278          1       1  ...       1           1          1      3        2   \n",
       "279          1       1  ...       1           2          1      2        2   \n",
       "\n",
       "     LIKES_DISCUSS  CLASSROOM  CUML_GPA  EXP_GPA  GRADE  \n",
       "0                1          2         1        1      1  \n",
       "1                3          2         2        3      1  \n",
       "2                1          1         2        2      1  \n",
       "3                2          1         3        2      1  \n",
       "4                2          1         2        2      1  \n",
       "..             ...        ...       ...      ...    ...  \n",
       "275              2          2         3        3      7  \n",
       "276              2          1         2        3      7  \n",
       "277              2          1         1        1      7  \n",
       "278              3          2         2        3      7  \n",
       "279              2          1         3        3      7  \n",
       "\n",
       "[280 rows x 31 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_train: \n",
      "     GRADE\n",
      "86       5\n",
      "137      2\n",
      "184      3\n",
      "5        2\n",
      "124      3\n",
      "..     ...\n",
      "188      3\n",
      "71       6\n",
      "106      7\n",
      "270      7\n",
      "102      7\n",
      "\n",
      "[224 rows x 1 columns]\n",
      "\n",
      "labels_test: \n",
      "     GRADE\n",
      "33       2\n",
      "108      6\n",
      "240      6\n",
      "259      6\n",
      "154      0\n",
      "9        0\n",
      "146      0\n",
      "203      4\n",
      "144      3\n",
      "155      0\n",
      "221      4\n",
      "92       7\n",
      "222      5\n",
      "209      4\n",
      "42       1\n",
      "210      4\n",
      "66       5\n",
      "90       6\n",
      "119      2\n",
      "142      1\n",
      "262      7\n",
      "268      7\n",
      "206      4\n",
      "238      5\n",
      "46       5\n",
      "77       7\n",
      "68       5\n",
      "75       7\n",
      "216      4\n",
      "277      7\n",
      "45       3\n",
      "111      2\n",
      "60       2\n",
      "217      4\n",
      "143      4\n",
      "30       5\n",
      "22       3\n",
      "24       2\n",
      "127      1\n",
      "176      2\n",
      "79       3\n",
      "264      7\n",
      "237      5\n",
      "120      1\n",
      "196      3\n",
      "245      6\n",
      "168      0\n",
      "6        5\n",
      "239      5\n",
      "73       6\n",
      "84       7\n",
      "56       5\n",
      "25       3\n",
      "97       6\n",
      "147      0\n",
      "19       3\n",
      "\n",
      "features_train: \n",
      "     AGE  GENDER  HS_TYPE  SCHOLARSHIP  WORK  ACTIVITY  PARTNER  SALARY  \\\n",
      "86     2       2        2            4     2         2        2       1   \n",
      "137    1       1        1            5     2         1        2       1   \n",
      "184    2       2        2            4     1         2        2       1   \n",
      "5      2       2        2            3     2         2        2       2   \n",
      "124    1       1        2            4     1         1        1       1   \n",
      "..   ...     ...      ...          ...   ...       ...      ...     ...   \n",
      "188    1       2        2            3     1         2        1       4   \n",
      "71     1       1        3            4     2         2        2       1   \n",
      "106    1       2        2            4     2         1        2       1   \n",
      "270    1       2        2            4     2         1        1       1   \n",
      "102    1       2        2            3     2         2        1       1   \n",
      "\n",
      "     TRANSPORT  LIVING  ...  IMPACT  ATTEND  PREP_STUDY  PREP_EXAM  NOTES  \\\n",
      "86           4       2  ...       1       1           1          1      3   \n",
      "137          1       1  ...       1       1           1          1      3   \n",
      "184          1       1  ...       2       1           1          1      3   \n",
      "5            1       1  ...       1       1           1          1      1   \n",
      "124          1       3  ...       1       1           2          2      3   \n",
      "..         ...     ...  ...     ...     ...         ...        ...    ...   \n",
      "188          1       1  ...       1       1           1          1      1   \n",
      "71           1       3  ...       1       1           1          1      2   \n",
      "106          1       2  ...       1       1           3          2      2   \n",
      "270          1       2  ...       1       1           1          1      3   \n",
      "102          1       2  ...       1       1           1          1      3   \n",
      "\n",
      "     LISTENS  LIKES_DISCUSS  CLASSROOM  CUML_GPA  EXP_GPA  \n",
      "86         2              3          2         5        4  \n",
      "137        1              3          1         2        4  \n",
      "184        2              2          1         4        3  \n",
      "5          2              1          2         4        4  \n",
      "124        3              2          1         3        3  \n",
      "..       ...            ...        ...       ...      ...  \n",
      "188        2              2          1         4        3  \n",
      "71         3              3          3         2        2  \n",
      "106        2              2          1         4        4  \n",
      "270        2              3          2         2        3  \n",
      "102        1              2          1         3        4  \n",
      "\n",
      "[224 rows x 30 columns]\n",
      "\n",
      "lfeatures_test: \n",
      "     AGE  GENDER  HS_TYPE  SCHOLARSHIP  WORK  ACTIVITY  PARTNER  SALARY  \\\n",
      "33     2       1        2            3     1         2        1       1   \n",
      "108    2       1        1            5     2         1        2       2   \n",
      "240    1       2        2            3     1         1        2       1   \n",
      "259    1       1        2            4     1         1        2       1   \n",
      "154    1       1        1            3     1         1        1       1   \n",
      "9      2       1        2            3     2         2        1       3   \n",
      "146    1       1        1            3     1         1        1       1   \n",
      "203    1       2        2            4     1         1        2       1   \n",
      "144    1       1        1            5     2         2        2       3   \n",
      "155    1       1        1            4     2         1        1       1   \n",
      "221    1       1        2            1     1         2        1       1   \n",
      "92     1       2        2            3     2         2        2       1   \n",
      "222    2       1        2            3     1         1        1       1   \n",
      "209    1       2        2            2     1         2        1       1   \n",
      "42     2       2        2            3     2         1        2       1   \n",
      "210    2       1        2            3     1         1        1       1   \n",
      "66     2       2        2            3     2         2        1       1   \n",
      "90     2       1        2            3     2         1        1       1   \n",
      "119    2       1        2            4     2         1        2       1   \n",
      "142    1       1        1            4     2         2        2       1   \n",
      "262    1       2        2            3     1         2        1       1   \n",
      "268    1       2        2            3     2         1        1       1   \n",
      "206    2       1        2            3     1         1        2       1   \n",
      "238    2       1        2            4     1         1        1       1   \n",
      "46     2       2        2            3     2         2        1       1   \n",
      "77     1       2        1            2     2         2        1       2   \n",
      "68     2       1        2            4     1         2        2       1   \n",
      "75     1       2        2            4     2         1        2       1   \n",
      "216    2       2        1            2     1         1        2       1   \n",
      "277    1       2        1            2     2         2        1       2   \n",
      "45     1       2        2            3     2         2        1       4   \n",
      "111    1       1        1            5     2         1        2       1   \n",
      "60     2       1        2            3     2         2        2       5   \n",
      "217    2       1        2            3     1         1        1       1   \n",
      "143    2       1        2            4     1         1        1       5   \n",
      "30     2       2        2            5     1         1        1       1   \n",
      "22     2       2        2            3     1         2        1       1   \n",
      "24     2       2        2            3     2         2        2       2   \n",
      "127    1       1        2            4     2         2        2       1   \n",
      "176    1       2        2            3     1         2        1       3   \n",
      "79     2       2        2            4     2         2        2       1   \n",
      "264    1       2        1            3     2         1        1       1   \n",
      "237    2       1        2            4     2         2        1       1   \n",
      "120    2       1        1            3     1         1        1       2   \n",
      "196    1       1        1            3     1         1        1       1   \n",
      "245    1       1        2            3     1         1        1       1   \n",
      "168    1       1        1            4     2         1        1       1   \n",
      "6      1       2        2            4     2         2        2       1   \n",
      "239    2       1        2            3     1         2        1       1   \n",
      "73     2       2        2            4     2         2        2       1   \n",
      "84     3       2        3            3     1         2        1       3   \n",
      "56     2       2        2            3     2         1        2       1   \n",
      "25     2       2        2            3     2         2        1       1   \n",
      "97     1       2        2            4     1         2        2       1   \n",
      "147    1       1        1            4     1         2        1       1   \n",
      "19     1       2        1            3     2         2        1       2   \n",
      "\n",
      "     TRANSPORT  LIVING  ...  IMPACT  ATTEND  PREP_STUDY  PREP_EXAM  NOTES  \\\n",
      "33           1       1  ...       1       1           1          1      1   \n",
      "108          2       1  ...       1       2           1          1      2   \n",
      "240          1       2  ...       1       1           1          1      2   \n",
      "259          1       3  ...       1       1           1          1      2   \n",
      "154          1       1  ...       1       1           1          1      2   \n",
      "9            4       2  ...       1       2           1          1      2   \n",
      "146          1       2  ...       1       1           2          1      2   \n",
      "203          1       1  ...       1       1           1          1      3   \n",
      "144          1       1  ...       1       1           2          1      3   \n",
      "155          2       1  ...       1       1           1          1      2   \n",
      "221          1       1  ...       1       1           1          1      2   \n",
      "92           1       1  ...       1       1           1          1      3   \n",
      "222          1       2  ...       1       1           1          1      2   \n",
      "209          1       1  ...       1       1           1          1      3   \n",
      "42           4       2  ...       1       1           1          1      2   \n",
      "210          1       1  ...       2       1           1          1      2   \n",
      "66           1       1  ...       1       1           1          1      3   \n",
      "90           2       3  ...       1       1           1          1      2   \n",
      "119          1       2  ...       1       1           1          2      2   \n",
      "142          1       1  ...       1       1           1          1      3   \n",
      "262          1       1  ...       1       1           1          1      3   \n",
      "268          1       2  ...       1       1           1          1      2   \n",
      "206          1       1  ...       1       1           1          1      3   \n",
      "238          1       1  ...       1       1           1          1      2   \n",
      "46           1       1  ...       1       1           1          1      2   \n",
      "77           2       2  ...       1       2           3          1      2   \n",
      "68           1       1  ...       1       2           1          1      2   \n",
      "75           1       3  ...       1       1           1          1      2   \n",
      "216          1       1  ...       1       1           1          1      2   \n",
      "277          1       2  ...       1       1           2          1      2   \n",
      "45           1       1  ...       1       1           1          1      2   \n",
      "111          1       2  ...       2       1           1          1      3   \n",
      "60           2       1  ...       1       1           1          1      1   \n",
      "217          1       1  ...       2       1           1          1      2   \n",
      "143          2       3  ...       1       1           2          1      2   \n",
      "30           1       2  ...       1       1           2          1      2   \n",
      "22           1       1  ...       1       1           1          2      3   \n",
      "24           1       1  ...       1       1           1          1      2   \n",
      "127          4       3  ...       1       1           1          1      3   \n",
      "176          1       1  ...       2       2           2          1      2   \n",
      "79           1       1  ...       3       1           1          1      3   \n",
      "264          1       1  ...       1       1           1          1      2   \n",
      "237          1       1  ...       1       1           2          1      2   \n",
      "120          2       3  ...       1       2           2          1      3   \n",
      "196          1       1  ...       1       1           1          1      3   \n",
      "245          1       2  ...       1       1           1          1      2   \n",
      "168          3       2  ...       1       1           1          1      2   \n",
      "6            1       3  ...       1       2           1          1      3   \n",
      "239          1       1  ...       1       1           1          1      2   \n",
      "73           1       2  ...       1       1           1          2      3   \n",
      "84           1       2  ...       1       1           3          3      3   \n",
      "56           1       1  ...       1       1           1          2      3   \n",
      "25           1       2  ...       1       1           1          1      2   \n",
      "97           1       3  ...       1       1           1          1      3   \n",
      "147          1       1  ...       1       1           1          1      2   \n",
      "19           2       2  ...       2       1           1          1      3   \n",
      "\n",
      "     LISTENS  LIKES_DISCUSS  CLASSROOM  CUML_GPA  EXP_GPA  \n",
      "33         3              2          2         2        3  \n",
      "108        3              1          2         3        3  \n",
      "240        2              2          1         2        2  \n",
      "259        3              2          2         2        2  \n",
      "154        2              2          2         3        2  \n",
      "9          2              2          2         1        2  \n",
      "146        2              2          1         2        2  \n",
      "203        2              3          2         4        3  \n",
      "144        2              3          1         5        4  \n",
      "155        2              2          2         3        2  \n",
      "221        3              2          2         2        2  \n",
      "92         2              3          3         2        2  \n",
      "222        1              2          1         3        3  \n",
      "209        1              3          2         2        2  \n",
      "42         1              3          1         2        3  \n",
      "210        3              1          1         3        2  \n",
      "66         2              2          3         5        4  \n",
      "90         3              2          3         4        2  \n",
      "119        2              3          1         3        3  \n",
      "142        3              2          1         4        3  \n",
      "262        2              2          3         2        2  \n",
      "268        1              2          1         3        4  \n",
      "206        2              2          1         2        2  \n",
      "238        2              3          2         4        3  \n",
      "46         2              3          1         4        2  \n",
      "77         2              2          2         1        1  \n",
      "68         2              3          2         4        3  \n",
      "75         2              2          2         4        1  \n",
      "216        1              2          1         4        3  \n",
      "277        2              2          1         1        1  \n",
      "45         2              2          1         4        3  \n",
      "111        1              3          3         4        3  \n",
      "60         3              3          1         2        1  \n",
      "217        3              1          1         3        2  \n",
      "143        1              2          1         5        3  \n",
      "30         3              3          3         5        4  \n",
      "22         1              2          3         3        3  \n",
      "24         1              3          2         4        4  \n",
      "127        2              2          1         2        2  \n",
      "176        1              2          1         2        2  \n",
      "79         3              3          2         4        4  \n",
      "264        2              3          1         4        4  \n",
      "237        2              2          1         3        2  \n",
      "120        3              3          2         2        2  \n",
      "196        2              2          2         2        3  \n",
      "245        2              2          1         3        2  \n",
      "168        2              2          1         2        2  \n",
      "6          3              3          3         4        4  \n",
      "239        2              2          2         4        3  \n",
      "73         2              3          1         5        3  \n",
      "84         3              3          3         5        4  \n",
      "56         2              3          3         5        4  \n",
      "25         1              3          2         1        2  \n",
      "97         2              2          1         3        3  \n",
      "147        2              2          2         3        2  \n",
      "19         2              2          3         2        3  \n",
      "\n",
      "[56 rows x 30 columns]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Extracting Labels\n",
    "\n",
    "columns = data_train.columns.to_list()\n",
    "columns_drop = columns.pop(-1)\n",
    "labels_train = data_train.drop(columns, axis=1)\n",
    "labels_test = data_test.drop(columns, axis=1)\n",
    "\n",
    "print(f\"labels_train: \\n{labels_train}\\n\")\n",
    "print(f\"labels_test: \\n{labels_test}\\n\")\n",
    "\n",
    "features_train = data_train.drop(['GRADE'], axis=1)\n",
    "features_test = data_test.drop(['GRADE'], axis=1)\n",
    "print(f\"features_train: \\n{features_train }\\n\")\n",
    "print(f\"lfeatures_test: \\n{features_test }\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ohenc = OneHotEncoder(sparse=False)\n",
    "\n",
    "ohenc_cols = ohenc.fit(features_train.drop(['AGE', 'SCHOLARSHIP', 'SALARY', 'MOTHER_EDU', 'FATHER_EDU', '#_SIBLINGS', 'STUDY_HRS', \n",
    "                'READ_FREQ', 'READ_FREQ_SCI', 'IMPACT', 'ATTEND', 'NOTES', 'LISTENS', 'LIKES_DISCUSS',\n",
    "                'CUML_GPA', 'EXP_GPA'], axis=1)).get_feature_names_out()\n",
    "\n",
    "\n",
    "ord_attribs = ['AGE', 'SCHOLARSHIP', 'SALARY', 'MOTHER_EDU', 'FATHER_EDU', '#_SIBLINGS', 'STUDY_HRS', \n",
    "                'READ_FREQ', 'READ_FREQ_SCI', 'IMPACT', 'ATTEND', 'NOTES', 'LISTENS', 'LIKES_DISCUSS',\n",
    "                'CUML_GPA', 'EXP_GPA']\n",
    "nom_attribs = ['GENDER', 'HS_TYPE', 'WORK', 'ACTIVITY', 'PARTNER', 'TRANSPORT', 'LIVING', 'PARENT_STATUS',\n",
    "                'MOTHER_JOB', 'FATHER_JOB', 'ATTEND_DEPT', 'PREP_STUDY', 'PREP_EXAM', 'CLASSROOM']\n",
    "\n",
    "pipe_cols = ['AGE' ,'GENDER_1' ,'GENDER_2' ,'HS_TYPE_1' ,'HS_TYPE_2' ,'HS_TYPE_3', 'SCHOLARSHIP' ,'WORK_1',\n",
    "             'WORK_2' ,'ACTIVITY_1', 'ACTIVITY_2', 'PARTNER_1' ,'PARTNER_2' ,'SALARY', 'TRANSPORT_1',\n",
    "             'TRANSPORT_2', 'TRANSPORT_3' ,'TRANSPORT_4' ,'LIVING_1' ,'LIVING_2', 'LIVING_3', 'LIVING_4',\n",
    "             'MOTHER_EDU', 'FATHER_EDU', '#_SIBLINGS', 'PARENT_STATUS_1' ,'PARENT_STATUS_2', 'PARENT_STATUS_3',\n",
    "             'MOTHER_JOB_1' ,'MOTHER_JOB_2' ,'MOTHER_JOB_3', 'MOTHER_JOB_4' ,'MOTHER_JOB_5' ,'FATHER_JOB_1' , \n",
    "             'FATHER_JOB_2', 'FATHER_JOB_3' ,'FATHER_JOB_4' ,'FATHER_JOB_5', 'STUDY_HRS', 'READ_FREQ',\n",
    "             'READ_FREQ_SCI',  'ATTEND_DEPT_1', 'ATTEND_DEPT_2', 'IMPACT', 'ATTEND','PREP_STUDY_1' ,'PREP_STUDY_2', \n",
    "             'PREP_STUDY_3', 'PREP_EXAM_1', 'PREP_EXAM_2' ,'PREP_EXAM_3' ,'NOTES', 'LISTENS', 'LIKES_DISCUSS', \n",
    "             'CLASSROOM_1' ,'CLASSROOM_2', 'CLASSROOM_3','CUML_GPA', 'EXP_GPA']\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "        (\"Nominal\", OneHotEncoder(), nom_attribs),\n",
    "        (\"Ordinal\", OrdinalEncoder(), ord_attribs)\n",
    "    ])\n",
    "\n",
    "data_prepared = pd.DataFrame(full_pipeline.fit_transform(features_train),columns=pipe_cols, index=features_train.index)\n",
    "#full_pipeline.fit_transform(data_train)\n",
    "#pd.DataFrame(full_pipeline.fit_transform(train_df),columns=train_df.columns, index=train_df.index)\\\n",
    "\n",
    "zero_data = np.zeros(shape=(len(data_prepared),1))\n",
    "\n",
    "#data_prepared.insert(37,\"MOTHER_EDU_6\", zero_data) #Inserting those columns that are not represented in the train data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the following four different models with their default hyperparameter values to be trained using the preprocessed data (0.5 * 4)\n",
    "labelTrainFlat = labels_train.values.ravel()\n",
    "# Gradient Boosting\n",
    "gradientBoosting = OneVsRestClassifier(GradientBoostingClassifier())\n",
    "gradientBoosting = gradientBoosting.fit(data_prepared, labelTrainFlat)\n",
    "\n",
    "# Decision Trees\n",
    "decisionTree = DecisionTreeClassifier()\n",
    "decisionTree = decisionTree.fit(data_prepared,labelTrainFlat)\n",
    "\n",
    "# Random Forests\n",
    "randomForest = RandomForestClassifier()\n",
    "randomForest = randomForest.fit(data_prepared,labelTrainFlat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters GradientBoosting: \n",
      "{'estimator__learning_rate': 0.46, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 7, 'estimator__n_estimators': 60}\n",
      "\n",
      "Best estimator GradientBoosting: \n",
      "OneVsRestClassifier(estimator=GradientBoostingClassifier(learning_rate=0.46,\n",
      "                                                         min_samples_leaf=5,\n",
      "                                                         min_samples_split=7,\n",
      "                                                         n_estimators=60))\n",
      "\n",
      "Best score GradientBoosting: \n",
      "0.6821056547619048\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parametersGradientBoosting = [\n",
    "    {'estimator__learning_rate': [0.44,0.45,0.46],'estimator__min_samples_leaf': [5,6,7],'estimator__min_samples_split': [7,8,9,10], 'estimator__n_estimators': [57,58,59,60]}\n",
    "]\n",
    "scoringX = {\"accuracy\": \"accuracy\", \"bal_accuracy\": \"balanced_accuracy\", \"F1_macro\": \"f1_macro\"}\n",
    "\n",
    "grid_searchGradientBoosting = GridSearchCV(gradientBoosting, parametersGradientBoosting, cv=4, scoring = scoringX, return_train_score=True, n_jobs=-1, refit='bal_accuracy')\n",
    "\n",
    "grid_searchGradientBoosting.fit(data_prepared, labelTrainFlat)\n",
    "\n",
    "print(f\"Best parameters GradientBoosting: \\n{grid_searchGradientBoosting.best_params_}\\n\")\n",
    "print(f\"Best estimator GradientBoosting: \\n{grid_searchGradientBoosting.best_estimator_}\\n\")\n",
    "print(f\"Best score GradientBoosting: \\n{grid_searchGradientBoosting.best_score_}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters KNeighbors: \n",
      "{'algorithm': 'auto', 'n_neighbors': 2, 'p': 2, 'weights': 'distance'}\n",
      "\n",
      "Best estimator KNeighbors: \n",
      "KNeighborsClassifier(n_neighbors=2, weights='distance')\n",
      "\n",
      "Best score KNeighbors: \n",
      "0.6709701178451177\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# KNeighbors\n",
    "kNeighbors = KNeighborsClassifier()\n",
    "kNeighbors = kNeighbors.fit(data_prepared,labelTrainFlat)\n",
    "\n",
    "parametersKNeighbors = [\n",
    "    {'n_neighbors': [1,2,3],'weights':['uniform', 'distance'],'algorithm':['auto'], 'p': [1,2,3]}\n",
    "]\n",
    "scoringX = {\"accuracy\": \"accuracy\", \"bal_accuracy\": \"balanced_accuracy\", \"F1_macro\": \"f1_macro\"}\n",
    "\n",
    "grid_searchKNeighbors = GridSearchCV(kNeighbors, parametersKNeighbors, cv=3, scoring = scoringX, return_train_score=True, n_jobs=-1, refit='bal_accuracy')\n",
    "\n",
    "grid_searchKNeighbors.fit(data_prepared, labelTrainFlat)\n",
    "\n",
    "print(f\"Best parameters KNeighbors: \\n{grid_searchKNeighbors.best_params_}\\n\")\n",
    "print(f\"Best estimator KNeighbors: \\n{grid_searchKNeighbors.best_estimator_}\\n\")\n",
    "print(f\"Best score KNeighbors: \\n{grid_searchKNeighbors.best_score_}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adamj\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\adamj\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\adamj\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\adamj\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters Logistic Regression: \n",
      "{'C': 2, 'multi_class': 'ovr', 'penalty': 'l2'}\n",
      "\n",
      "Best estimator Logistic Regression: \n",
      "LogisticRegression(C=2, multi_class='ovr')\n",
      "\n",
      "Best score Logistic Regression: \n",
      "0.5035563973063972\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LogisticRegression\n",
    "logisticRegression = LogisticRegression()\n",
    "logisticRegression = logisticRegression.fit(data_prepared,labelTrainFlat)\n",
    "\n",
    "parametersLogisticRegression = [\n",
    "    {'multi_class': ['ovr'],'penalty':['none','l2'], 'C': [1,2,3]}\n",
    "]\n",
    "scoringX = {\"accuracy\": \"accuracy\", \"bal_accuracy\": \"balanced_accuracy\", \"F1_macro\": \"f1_macro\"}\n",
    "\n",
    "grid_searchLogisticRegression = GridSearchCV(logisticRegression, parametersLogisticRegression, cv=3, scoring = scoringX, return_train_score=True, n_jobs=-1, refit='bal_accuracy')\n",
    "\n",
    "grid_searchLogisticRegression.fit(data_prepared, labelTrainFlat)\n",
    "\n",
    "print(f\"Best parameters Logistic Regression: \\n{grid_searchLogisticRegression.best_params_}\\n\")\n",
    "print(f\"Best estimator Logistic Regression: \\n{grid_searchLogisticRegression.best_estimator_}\\n\")\n",
    "print(f\"Best score Logistic Regression: \\n{grid_searchLogisticRegression.best_score_}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters DecisionTree: \n",
      "{'max_depth': 4, 'min_samples_leaf': 4, 'min_samples_split': 2}\n",
      "\n",
      "Best estimator DecisionTree: \n",
      "DecisionTreeClassifier(max_depth=4, min_samples_leaf=4)\n",
      "\n",
      "Best score DecisionTree: \n",
      "0.33188131313131314\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adamj\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "36 fits failed out of a total of 108.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "36 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\adamj\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\adamj\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"C:\\Users\\adamj\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 250, in fit\n",
      "    raise ValueError(\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\adamj\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan 0.16954955 0.16954955        nan 0.16954955 0.16954955\n",
      "        nan 0.16954955 0.16954955        nan 0.20108108 0.20108108\n",
      "        nan 0.20108108 0.20108108        nan 0.20108108 0.20108108\n",
      "        nan 0.25891892 0.25891892        nan 0.26342342 0.26342342\n",
      "        nan 0.25459459 0.25459459        nan 0.33063063 0.33063063\n",
      "        nan 0.32612613 0.32612613        nan 0.32162162 0.32162162]\n",
      "  warnings.warn(\n",
      "C:\\Users\\adamj\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the train scores are non-finite: [       nan 0.22547353 0.22547353        nan 0.22547353 0.22547353\n",
      "        nan 0.22547353 0.22547353        nan 0.32143177 0.32143177\n",
      "        nan 0.32143177 0.32143177        nan 0.32143177 0.32143177\n",
      "        nan 0.42851603 0.42851603        nan 0.41956749 0.41956749\n",
      "        nan 0.41734526 0.41734526        nan 0.54456376 0.54456376\n",
      "        nan 0.53561521 0.53561521        nan 0.52444444 0.52444444]\n",
      "  warnings.warn(\n",
      "C:\\Users\\adamj\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan 0.16527778 0.16527778        nan 0.16527778 0.16527778\n",
      "        nan 0.16527778 0.16527778        nan 0.19583333 0.19583333\n",
      "        nan 0.19583333 0.19583333        nan 0.19583333 0.19583333\n",
      "        nan 0.25273569 0.25273569        nan 0.25690236 0.25690236\n",
      "        nan 0.2493266  0.2493266         nan 0.33188131 0.33188131\n",
      "        nan 0.32563131 0.32563131        nan 0.32146465 0.32146465]\n",
      "  warnings.warn(\n",
      "C:\\Users\\adamj\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the train scores are non-finite: [       nan 0.21158104 0.21158104        nan 0.21158104 0.21158104\n",
      "        nan 0.21158104 0.21158104        nan 0.31030876 0.31030876\n",
      "        nan 0.31030876 0.31030876        nan 0.31030876 0.31030876\n",
      "        nan 0.42131196 0.42131196        nan 0.41251566 0.41251566\n",
      "        nan 0.41132519 0.41132519        nan 0.54244217 0.54244217\n",
      "        nan 0.53367723 0.53367723        nan 0.52293814 0.52293814]\n",
      "  warnings.warn(\n",
      "C:\\Users\\adamj\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan 0.07495072 0.07495072        nan 0.07495072 0.07495072\n",
      "        nan 0.07495072 0.07495072        nan 0.13269273 0.13269273\n",
      "        nan 0.13269273 0.13269273        nan 0.13269273 0.13269273\n",
      "        nan 0.20221925 0.20219223        nan 0.20520266 0.20520266\n",
      "        nan 0.19733457 0.19733457        nan 0.3046655  0.3046655\n",
      "        nan 0.30049553 0.30049553        nan 0.2879162  0.2879162 ]\n",
      "  warnings.warn(\n",
      "C:\\Users\\adamj\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the train scores are non-finite: [       nan 0.1013433  0.1013433         nan 0.1013433  0.1013433\n",
      "        nan 0.1013433  0.1013433         nan 0.20890103 0.20890103\n",
      "        nan 0.20890103 0.20890103        nan 0.20890103 0.20890103\n",
      "        nan 0.35379002 0.35379002        nan 0.34626887 0.34626887\n",
      "        nan 0.34275374 0.34275374        nan 0.50939947 0.50939947\n",
      "        nan 0.5030768  0.5030768         nan 0.4848833  0.4848833 ]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "parametersDecisionTree = [\n",
    "    {'max_depth': [1,2,3,4], 'min_samples_leaf': [4,5,6], 'min_samples_split': [1,2,3]}\n",
    "]\n",
    "\n",
    "grid_searchDecisionTree = GridSearchCV(decisionTree, parametersDecisionTree, cv=3, scoring = scoringX, return_train_score=True, n_jobs=-1, refit='bal_accuracy')\n",
    "\n",
    "grid_searchDecisionTree.fit(data_prepared, labelTrainFlat)\n",
    "\n",
    "print(f\"Best parameters DecisionTree: \\n{grid_searchDecisionTree.best_params_}\\n\")\n",
    "print(f\"Best estimator DecisionTree: \\n{grid_searchDecisionTree.best_estimator_}\\n\")\n",
    "print(f\"Best score DecisionTree: \\n{grid_searchDecisionTree.best_score_}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters RandomForest: \n",
      "{'bootstrap': False, 'max_depth': 12, 'min_samples_split': 2, 'n_estimators': 150}\n",
      "\n",
      "Best estimator RandomForest: \n",
      "RandomForestClassifier(bootstrap=False, max_depth=12, n_estimators=150)\n",
      "\n",
      "Best score RandomForest: \n",
      "0.6945684523809523\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parametersRandomForest = [\n",
    "    {'n_estimators': [145,150,155,190],'max_depth': [10,12], 'bootstrap': [True, False],\n",
    "     'min_samples_split': [0.05,2]}\n",
    "]\n",
    "\n",
    "grid_searchRandomForest = GridSearchCV(randomForest, parametersRandomForest, cv=4, scoring = scoringX, return_train_score=True, n_jobs=-1, refit='bal_accuracy')\n",
    "\n",
    "grid_searchRandomForest.fit(data_prepared, labelTrainFlat)\n",
    "\n",
    "print(f\"Best parameters RandomForest: \\n{grid_searchRandomForest.best_params_}\\n\")\n",
    "\n",
    "print(f\"Best estimator RandomForest: \\n{grid_searchRandomForest.best_estimator_}\\n\")\n",
    "\n",
    "print(f\"Best score RandomForest: \\n{grid_searchRandomForest.best_score_}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Test Accuracy for GradientBoosting: \n",
      "[0.66517857 0.65625    0.64732143 0.65625    0.66517857 0.64732143\n",
      " 0.65178571 0.65178571 0.67857143 0.66071429 0.65625    0.65178571\n",
      " 0.65625    0.64732143 0.65178571 0.66517857 0.65625    0.65625\n",
      " 0.64732143 0.64732143 0.64732143 0.65625    0.65178571 0.65625\n",
      " 0.66071429 0.65625    0.64732143 0.65178571 0.66071429 0.65625\n",
      " 0.65178571 0.65625    0.62946429 0.625      0.63839286 0.62946429\n",
      " 0.63392857 0.61160714 0.63392857 0.64732143 0.625      0.62946429\n",
      " 0.65178571 0.63839286 0.64285714 0.62946429 0.63392857 0.65178571\n",
      " 0.64732143 0.65625    0.66517857 0.65625    0.66517857 0.65625\n",
      " 0.64285714 0.66071429 0.65178571 0.65625    0.65625    0.65625\n",
      " 0.64732143 0.65178571 0.65178571 0.65178571 0.65178571 0.63839286\n",
      " 0.64285714 0.64732143 0.65178571 0.63392857 0.63392857 0.63839286\n",
      " 0.65178571 0.63839286 0.63839286 0.63839286 0.64732143 0.63392857\n",
      " 0.64285714 0.63839286 0.65178571 0.63839286 0.63839286 0.63839286\n",
      " 0.63392857 0.64285714 0.63392857 0.63839286 0.64732143 0.65178571\n",
      " 0.65625    0.64732143 0.64732143 0.63392857 0.64732143 0.64285714\n",
      " 0.66517857 0.66517857 0.67857143 0.67857143 0.66517857 0.66964286\n",
      " 0.66517857 0.67410714 0.66964286 0.65625    0.66517857 0.65625\n",
      " 0.67857143 0.65625    0.66964286 0.66071429 0.66071429 0.64732143\n",
      " 0.65178571 0.65625    0.66517857 0.65178571 0.65625    0.64732143\n",
      " 0.66071429 0.64732143 0.65178571 0.64285714 0.66071429 0.65178571\n",
      " 0.65625    0.65178571 0.63392857 0.63839286 0.64285714 0.62946429\n",
      " 0.63839286 0.63839286 0.62946429 0.64285714 0.62946429 0.64285714\n",
      " 0.64285714 0.64732143 0.625      0.63392857 0.63392857 0.625     ]\n",
      "\n",
      "Balanced Test Accuracy for GradientBoosting: \n",
      "[0.6687128  0.65978423 0.65085565 0.65904018 0.66741071 0.64955357\n",
      " 0.65531994 0.65531994 0.68098958 0.66424851 0.66015625 0.65643601\n",
      " 0.66034226 0.64955357 0.65401786 0.6687128  0.65866815 0.65811012\n",
      " 0.64899554 0.64899554 0.64973958 0.65811012 0.65420387 0.65866815\n",
      " 0.66313244 0.65811012 0.64899554 0.65420387 0.66313244 0.65811012\n",
      " 0.65345982 0.65866815 0.63206845 0.6264881  0.63988095 0.63151042\n",
      " 0.63541667 0.61216518 0.63541667 0.65029762 0.62760417 0.63095238\n",
      " 0.65606399 0.63988095 0.64620536 0.63095238 0.63616071 0.65531994\n",
      " 0.64899554 0.65662202 0.66629464 0.65736607 0.66629464 0.65792411\n",
      " 0.64434524 0.66127232 0.65271577 0.65662202 0.65736607 0.65736607\n",
      " 0.64825149 0.65271577 0.65401786 0.65290179 0.65234375 0.63895089\n",
      " 0.64397321 0.64787946 0.65234375 0.63504464 0.63504464 0.63950893\n",
      " 0.65234375 0.63950893 0.63950893 0.63895089 0.64787946 0.63448661\n",
      " 0.64397321 0.63950893 0.65383185 0.64099702 0.64099702 0.64025298\n",
      " 0.63578869 0.64415923 0.63578869 0.64043899 0.64862351 0.65438988\n",
      " 0.65885417 0.64936756 0.64936756 0.63578869 0.6499256  0.64490327\n",
      " 0.66889881 0.66759673 0.68043155 0.68210565 0.66815476 0.67206101\n",
      " 0.66815476 0.67708333 0.67392113 0.65922619 0.66815476 0.66034226\n",
      " 0.68098958 0.65978423 0.67261905 0.66369048 0.66052827 0.64769345\n",
      " 0.65290179 0.65662202 0.6655506  0.65215774 0.65736607 0.64825149\n",
      " 0.66052827 0.64825149 0.65345982 0.64434524 0.66052827 0.65215774\n",
      " 0.65792411 0.65271577 0.63802083 0.64099702 0.64750744 0.63318452\n",
      " 0.64099702 0.64099702 0.63262649 0.64750744 0.63206845 0.64620536\n",
      " 0.64750744 0.65066964 0.62890625 0.63653274 0.63783482 0.62872024]\n",
      "\n",
      "Mean F1 Macro for GradientBoosting: \n",
      "[0.66058693 0.6521728  0.6404364  0.65367001 0.66084193 0.64447951\n",
      " 0.65020029 0.64859696 0.67410206 0.6557759  0.65073147 0.64745698\n",
      " 0.64933498 0.64609086 0.64747661 0.6599853  0.65586665 0.65394395\n",
      " 0.64598071 0.64331011 0.64358885 0.65289388 0.64957363 0.65316189\n",
      " 0.65965843 0.65394395 0.64598071 0.64776936 0.65681858 0.65399279\n",
      " 0.6488242  0.65157181 0.62073097 0.61972651 0.63414255 0.62277697\n",
      " 0.63035554 0.60443586 0.62910003 0.64364277 0.61770832 0.62735518\n",
      " 0.64814412 0.63393796 0.63724978 0.62615012 0.62993718 0.64793149\n",
      " 0.64879516 0.65661589 0.66598013 0.65452278 0.66554353 0.6549719\n",
      " 0.64287795 0.66053572 0.65135319 0.65725978 0.65690803 0.65333449\n",
      " 0.64687745 0.65312225 0.65182787 0.65193609 0.64812489 0.63596112\n",
      " 0.64230163 0.64759688 0.64750485 0.63348191 0.63260336 0.63668266\n",
      " 0.64748569 0.63755949 0.63593727 0.63688682 0.64125667 0.63196466\n",
      " 0.64355276 0.63703778 0.65179878 0.63956678 0.63907606 0.63914363\n",
      " 0.63279794 0.64052361 0.63270753 0.63696879 0.64724075 0.6534078\n",
      " 0.65768066 0.6489131  0.64648109 0.63382685 0.64739196 0.64265061\n",
      " 0.6630076  0.66559617 0.67949715 0.67860062 0.66511021 0.66988312\n",
      " 0.66908765 0.67593905 0.66863983 0.65722872 0.66906723 0.65826199\n",
      " 0.67916926 0.65662153 0.66969672 0.6635657  0.65968408 0.64241644\n",
      " 0.64887136 0.65382872 0.66448622 0.65033106 0.65340433 0.64468265\n",
      " 0.65920894 0.64270807 0.65180321 0.64072899 0.65920894 0.64992256\n",
      " 0.65601518 0.64921562 0.63065636 0.635727   0.64238784 0.62774241\n",
      " 0.63530418 0.63664275 0.62774376 0.63793419 0.62828537 0.63931956\n",
      " 0.64226151 0.64180417 0.62258145 0.63082849 0.63069261 0.61920682]\n",
      "\n",
      "Mean Test Accuracy for Decision Trees: \n",
      "[       nan 0.16954955 0.16954955        nan 0.16954955 0.16954955\n",
      "        nan 0.16954955 0.16954955        nan 0.20108108 0.20108108\n",
      "        nan 0.20108108 0.20108108        nan 0.20108108 0.20108108\n",
      "        nan 0.25891892 0.25891892        nan 0.26342342 0.26342342\n",
      "        nan 0.25459459 0.25459459        nan 0.33063063 0.33063063\n",
      "        nan 0.32612613 0.32612613        nan 0.32162162 0.32162162]\n",
      "\n",
      "Balanced Test Accuracy for Decision Trees: \n",
      "[       nan 0.16527778 0.16527778        nan 0.16527778 0.16527778\n",
      "        nan 0.16527778 0.16527778        nan 0.19583333 0.19583333\n",
      "        nan 0.19583333 0.19583333        nan 0.19583333 0.19583333\n",
      "        nan 0.25273569 0.25273569        nan 0.25690236 0.25690236\n",
      "        nan 0.2493266  0.2493266         nan 0.33188131 0.33188131\n",
      "        nan 0.32563131 0.32563131        nan 0.32146465 0.32146465]\n",
      "\n",
      "Mean F1 Macro for Decision Trees: \n",
      "[       nan 0.07495072 0.07495072        nan 0.07495072 0.07495072\n",
      "        nan 0.07495072 0.07495072        nan 0.13269273 0.13269273\n",
      "        nan 0.13269273 0.13269273        nan 0.13269273 0.13269273\n",
      "        nan 0.20221925 0.20219223        nan 0.20520266 0.20520266\n",
      "        nan 0.19733457 0.19733457        nan 0.3046655  0.3046655\n",
      "        nan 0.30049553 0.30049553        nan 0.2879162  0.2879162 ]\n",
      "\n",
      "Mean Test Accuracy for Random Forests: \n",
      "[0.5625     0.58928571 0.5625     0.58928571 0.66517857 0.64285714\n",
      " 0.62946429 0.65625    0.59375    0.58482143 0.61607143 0.58928571\n",
      " 0.64285714 0.64732143 0.63392857 0.67410714 0.59821429 0.59821429\n",
      " 0.58035714 0.60267857 0.66071429 0.65625    0.64732143 0.67857143\n",
      " 0.58928571 0.59821429 0.58482143 0.60267857 0.6875     0.6875\n",
      " 0.66517857 0.6875    ]\n",
      "\n",
      "Balanced Test Accuracy for Random Forests: \n",
      "[0.56752232 0.59281994 0.56733631 0.59635417 0.67280506 0.64657738\n",
      " 0.6343006  0.66164435 0.59951637 0.58984375 0.62146577 0.59375\n",
      " 0.64825149 0.65327381 0.64229911 0.67912946 0.60342262 0.60584077\n",
      " 0.58426339 0.60602679 0.66778274 0.66183036 0.65364583 0.68489583\n",
      " 0.59430804 0.60602679 0.59188988 0.60956101 0.69401042 0.69456845\n",
      " 0.67131696 0.69196429]\n",
      "\n",
      "Mean F1 Macro for Random Forests: \n",
      "[0.55580546 0.58084882 0.55105371 0.57870488 0.6523697  0.6331015\n",
      " 0.61509239 0.64324832 0.58763568 0.57405233 0.60549182 0.57762503\n",
      " 0.62731403 0.64106308 0.62395134 0.664833   0.59858569 0.59558443\n",
      " 0.57636387 0.59474607 0.64945762 0.64713632 0.63651652 0.67333043\n",
      " 0.58474312 0.58715349 0.57929293 0.59254677 0.68193344 0.67693234\n",
      " 0.65753661 0.68146379]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the grid search cross-validation results listing the above mentioned evaluation methods (3)\n",
    "cross_val_resultsGB = grid_searchGradientBoosting.cv_results_\n",
    "cross_val_resultsDT = grid_searchDecisionTree.cv_results_\n",
    "cross_val_resultsRF = grid_searchRandomForest.cv_results_\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Mean Test Accuracy for GradientBoosting: \\n{cross_val_resultsGB['mean_test_accuracy']}\\n\")\n",
    "print(f\"Balanced Test Accuracy for GradientBoosting: \\n{cross_val_resultsGB['mean_test_bal_accuracy']}\\n\")\n",
    "print(f\"Mean F1 Macro for GradientBoosting: \\n{cross_val_resultsGB['mean_test_F1_macro']}\\n\")\n",
    "\n",
    "#DTC\n",
    "print(f\"Mean Test Accuracy for Decision Trees: \\n{cross_val_resultsDT['mean_test_accuracy']}\\n\")\n",
    "print(f\"Balanced Test Accuracy for Decision Trees: \\n{cross_val_resultsDT['mean_test_bal_accuracy']}\\n\")\n",
    "print(f\"Mean F1 Macro for Decision Trees: \\n{cross_val_resultsDT['mean_test_F1_macro']}\\n\")\n",
    "\n",
    "#RFC\n",
    "print(f\"Mean Test Accuracy for Random Forests: \\n{cross_val_resultsRF['mean_test_accuracy']}\\n\")\n",
    "print(f\"Balanced Test Accuracy for Random Forests: \\n{cross_val_resultsRF['mean_test_bal_accuracy']}\\n\")\n",
    "print(f\"Mean F1 Macro for Random Forests: \\n{cross_val_resultsRF['mean_test_F1_macro']}\\n\")\n",
    "\n",
    "#NB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy Prediction: \n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1]\n",
      "\n",
      "Dummy Score: \n",
      "0.13839285714285715\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use a dummy classifier to identify a simple baseline (i.e., a majority class baseline) so that you can compare your prediction results (3)\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy_clf.fit(data_prepared, labelTrainFlat)\n",
    "DummyClassifier(strategy='most_frequent')\n",
    "print(f\"Dummy Prediction: \\n{dummy_clf.predict(data_prepared)}\\n\") \n",
    "print(f\"Dummy Score: \\n{dummy_clf.score(data_prepared, labelTrainFlat)}\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      1.00      0.71         6\n",
      "           1       0.00      0.00      0.00         4\n",
      "           2       0.80      0.67      0.73         6\n",
      "           3       0.71      0.71      0.71         7\n",
      "           4       0.88      0.88      0.88         8\n",
      "           5       0.83      0.50      0.62        10\n",
      "           6       0.50      0.43      0.46         7\n",
      "           7       0.86      0.75      0.80         8\n",
      "\n",
      "    accuracy                           0.64        56\n",
      "   macro avg       0.64      0.62      0.61        56\n",
      "weighted avg       0.69      0.64      0.65        56\n",
      "\n",
      "[[6 0 0 0 0 0 0 0]\n",
      " [3 0 0 0 0 0 0 1]\n",
      " [0 1 4 0 0 1 0 0]\n",
      " [0 1 1 5 0 0 0 0]\n",
      " [1 0 0 0 7 0 0 0]\n",
      " [0 2 0 0 1 5 2 0]\n",
      " [1 2 0 1 0 0 3 0]\n",
      " [0 0 0 1 0 0 1 6]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ohenc_test = OneHotEncoder(sparse=False)\n",
    "ohenc_cols_test = ohenc.fit(features_test.drop(['AGE', 'SCHOLARSHIP', 'SALARY', 'MOTHER_EDU', 'FATHER_EDU', '#_SIBLINGS', 'STUDY_HRS', \n",
    "                'READ_FREQ', 'READ_FREQ_SCI', 'IMPACT', 'ATTEND', 'NOTES', 'LISTENS', 'LIKES_DISCUSS',\n",
    "                'CUML_GPA', 'EXP_GPA'], axis=1)).get_feature_names_out()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "missing_cols_from_test = ['MOTHER_JOB_5', 'LIVING_4', 'FATHER_EDU_6']\n",
    "missing_cols_from_train = ['MOTHER_EDU_6']\n",
    "\n",
    "ord_attribs_test = ['AGE', 'SCHOLARSHIP', 'SALARY', 'MOTHER_EDU', 'FATHER_EDU', '#_SIBLINGS', 'STUDY_HRS', \n",
    "                'READ_FREQ', 'READ_FREQ_SCI', 'IMPACT', 'ATTEND', 'NOTES', 'LISTENS', 'LIKES_DISCUSS',\n",
    "                'CUML_GPA', 'EXP_GPA']\n",
    "nom_attribs_test = ['GENDER', 'HS_TYPE', 'WORK', 'ACTIVITY', 'PARTNER', 'TRANSPORT', 'LIVING', 'PARENT_STATUS',\n",
    "                'MOTHER_JOB', 'FATHER_JOB', 'ATTEND_DEPT', 'PREP_STUDY', 'PREP_EXAM', 'CLASSROOM']\n",
    "\n",
    "pipe_cols_test = ['AGE' ,'GENDER_1' ,'GENDER_2' ,'HS_TYPE_1' ,'HS_TYPE_2' ,'HS_TYPE_3', 'SCHOLARSHIP' ,'WORK_1',\n",
    "             'WORK_2' ,'ACTIVITY_1', 'ACTIVITY_2', 'PARTNER_1' ,'PARTNER_2' ,'SALARY', 'TRANSPORT_1',\n",
    "             'TRANSPORT_2', 'TRANSPORT_3' ,'TRANSPORT_4' ,'LIVING_1' ,'LIVING_2', 'LIVING_3',\n",
    "             'MOTHER_EDU', 'FATHER_EDU', '#_SIBLINGS', 'PARENT_STATUS_1' ,'PARENT_STATUS_2', 'PARENT_STATUS_3',\n",
    "             'MOTHER_JOB_1' ,'MOTHER_JOB_2' ,'MOTHER_JOB_3', 'MOTHER_JOB_4' ,'FATHER_JOB_1' , \n",
    "             'FATHER_JOB_2', 'FATHER_JOB_3' ,'FATHER_JOB_4' ,'FATHER_JOB_5', 'STUDY_HRS', 'READ_FREQ',\n",
    "             'READ_FREQ_SCI',  'ATTEND_DEPT_1', 'ATTEND_DEPT_2', 'IMPACT', 'ATTEND','PREP_STUDY_1' ,'PREP_STUDY_2', \n",
    "             'PREP_STUDY_3', 'PREP_EXAM_1', 'PREP_EXAM_2' ,'PREP_EXAM_3' ,'NOTES', 'LISTENS', 'LIKES_DISCUSS', \n",
    "             'CLASSROOM_1' ,'CLASSROOM_2', 'CLASSROOM_3','CUML_GPA', 'EXP_GPA']\n",
    "\n",
    "\n",
    "full_pipeline_test = ColumnTransformer([\n",
    "        (\"Nominal\", OneHotEncoder(), nom_attribs),\n",
    "        (\"Ordinal\", OrdinalEncoder(), ord_attribs)\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "data_prepared_test = pd.DataFrame(full_pipeline_test.fit_transform(features_test),columns=pipe_cols_test, index=features_test.index)\n",
    "missing_cols_df = pd.DataFrame(0, index=np.arange(56), columns=missing_cols_from_test)\n",
    "res = list(set(ohenc_cols).difference(set(ohenc_cols_test)))\n",
    "\n",
    "zero_data = np.zeros(shape=(len(data_prepared_test),1))\n",
    "\n",
    "\n",
    "data_prepared_test.insert(31,\"MOTHER_JOB_5\", zero_data) #Inserting those columns that are not represented in the test data\n",
    "data_prepared_test.insert(21,\"LIVING_4\", zero_data)\n",
    "\n",
    "#data_prepared_test.insert(43,\"FATHER_EDU_6\", zero_data)\n",
    "\n",
    "\n",
    "\n",
    "# obtain predictions on test data using the best model from GridSearchCV (i.e., .best_estimator_) (2)\n",
    "predictions_test = grid_searchRandomForest.best_estimator_.predict(data_prepared_test)\n",
    "\n",
    "# generate the classification report and the confusion matrix for test predictions (3)\n",
    "print(classification_report(labels_test.values.ravel(),predictions_test))\n",
    "print(confusion_matrix(labels_test, predictions_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "af4e0a0d28143374aa5d305078a03b698686e6b7df811af5a427e46b8cc107ac"
  },
  "kernelspec": {
   "display_name": "Python 3.10.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
